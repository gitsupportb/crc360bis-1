# CRC360 - Comprehensive Risk and Compliance 360¬∞ Dashboard

![CRC360 Logo](public/logo.png)

## üåü Executive Summary

CRC360 (Comprehensive Risk and Compliance 360¬∞) is an enterprise-grade, integrated risk management and compliance platform specifically designed for BCP Securities Services. This sophisticated web application serves as a unified command center that consolidates multiple specialized modules for comprehensive risk assessment, regulatory compliance monitoring, secure document management, and advanced financial reporting.

The platform represents a complete digital transformation solution that addresses the complex regulatory landscape of financial services, providing real-time monitoring, automated compliance tracking, and sophisticated risk analytics in a single, cohesive interface.

## üèóÔ∏è System Architecture Overview

CRC360 employs a modern, scalable architecture built on Next.js 15 with React 19, utilizing a hybrid approach that combines:

- **Frontend**: Server-side rendered React components with client-side interactivity
- **Backend**: Integrated Express.js server with Next.js API routes
- **Database**: SQLite for document metadata, in-memory storage for real-time data
- **File Processing**: Advanced document parsing and analysis engines
- **Security**: Multi-layered authentication and authorization system

### Core Technology Stack

| Component | Technology | Version | Purpose |
|-----------|------------|---------|---------|
| **Frontend Framework** | Next.js | 15.2.4 | Server-side rendering, routing, and optimization |
| **UI Library** | React | 19.x | Component-based user interface |
| **Language** | TypeScript | 5.x | Type-safe development |
| **Styling** | Tailwind CSS | 3.4.17 | Utility-first CSS framework |
| **UI Components** | Radix UI | Various | Accessible, unstyled components |
| **Backend** | Express.js | 4.21.2 | API server and middleware |
| **File Processing** | Multiple | - | XLSX, PDF-Parse, XML2JS, Multer |
| **Database** | SQLite | - | Document metadata storage |
| **Build Tools** | PostCSS, ESLint | - | Code quality and optimization |

## üöÄ Comprehensive Feature Matrix

### üè† **Main Dashboard - Central Command Center**

The main dashboard serves as the primary entry point and navigation hub for the entire CRC360 ecosystem:

#### **Visual Design & UX**
- **Modern Card-Based Layout**: Interactive cards with hover effects and smooth transitions
- **Responsive Grid System**: Adaptive layout that works seamlessly across desktop, tablet, and mobile devices
- **Professional Color Scheme**: Orange-themed design (#FF6B35) reflecting BCP Securities Services branding
- **Intuitive Navigation**: Clear visual hierarchy with iconography and descriptive text
- **Real-time Status Indicators**: Live system health and module availability indicators

#### **Navigation Features**
- **Unified Header**: Consistent navigation bar with BCP Securities Services logo
- **Module Cards**: Four primary modules (Rep Watch, AML Center, Doc Secure, R-Sense)
- **Quick Access Links**: Direct navigation to frequently used features
- **Breadcrumb Navigation**: Context-aware navigation trails
- **Mobile-Optimized Menu**: Collapsible navigation for mobile devices

#### **Technical Implementation**
- **Server-Side Rendering**: Fast initial page loads with SEO optimization
- **Client-Side Routing**: Smooth transitions between modules
- **State Management**: Centralized state for user preferences and session data
- **Error Boundaries**: Graceful error handling and recovery

### üìä **Rep Watch - Advanced Risk Monitoring & Reporting**

Rep Watch is a comprehensive reporting and risk monitoring system that provides real-time insights into regulatory compliance and risk metrics:

#### **Dashboard Features**
- **Interactive Calendar Heatmap**: Visual representation of reporting activities and deadlines
- **Progress Tracking**: Real-time completion status for all regulatory reports
- **Monthly Task Management**: Dedicated interface for monthly presentation preparation
- **Advanced Analytics**: Plotly.js-powered charts and visualizations
- **Export Capabilities**: PNG image export for presentations and documentation

#### **Reporting Categories**

**Category I: Situation comptable et √©tats annexes (26 reports)**
- Monthly, quarterly, and annual financial statements
- T√©l√©transmission and file-based submissions
- Automated deadline tracking and notifications
- Progress indicators and completion status

**Category II: Etats de synth√®se et documents compl√©mentaires (23 reports)**
- Semi-annual and annual synthesis statements
- Comprehensive financial documentation
- Regulatory compliance tracking
- Multi-format submission support

**Category III: Etats relatifs √† la r√©glementation prudentielle (19 reports)**
- Prudential regulation compliance reports
- Stress testing documentation
- Risk assessment reports
- Regulatory capital calculations

#### **Advanced Features**
- **Smart Date Selection**: Complete day/month/year picker with leap year support
- **Deadline Management**: Automated calculation of submission deadlines
- **Status Tracking**: Visual indicators (Completed, Pending, Overdue)
- **Priority Levels**: Critical, High, Medium, Low based on deadline proximity
- **File Upload System**: Drag-and-drop interface with category-based filtering
- **Upload Analytics**: Comprehensive tracking and statistics
- **Export Functionality**: CSV export for external analysis

#### **Technical Specifications**
- **File Format**: Standalone HTML with embedded JavaScript
- **Dependencies**: Plotly.js for charts, html2canvas for image export
- **Storage**: Local storage for progress persistence
- **Performance**: Optimized for large datasets with lazy loading
- **Browser Compatibility**: Modern browsers with ES6+ support

### üõ°Ô∏è **AML Center - Anti-Money Laundering Compliance**

The AML Center is a sophisticated compliance monitoring system designed to handle sanctions screening, client risk assessment, and regulatory compliance:

#### **Core Functionality**

**Sanctions List Management**
- **PDF Processing**: Advanced text extraction and parsing from sanctions lists
- **XML Data Processing**: Structured data import from regulatory sources
- **Entity Recognition**: Automatic identification of individuals vs. entities
- **Duplicate Detection**: Intelligent deduplication algorithms
- **Search Capabilities**: Advanced filtering and search functionality

**Client Risk Assessment**
- **Excel File Processing**: Automated analysis of client risk assessment spreadsheets
- **Risk Categorization**: Five-category risk assessment framework:
  - Zone g√©ographique (Geographic Zone)
  - Caract√©ristiques du client (Client Characteristics)
  - R√©putation du client (Client Reputation)
  - Nature produits/op√©rations (Product/Operation Nature)
  - Canal de distribution (Distribution Channel)
- **Risk Level Calculation**: Automated risk scoring and classification
- **Progress Tracking**: Visual progress indicators and completion status

**Data Processing Engine**
- **Multi-format Support**: PDF, XML, and Excel file processing
- **Advanced Parsing**: Regex-based text extraction with error handling
- **Data Validation**: Comprehensive input validation and sanitization
- **Merge Cell Handling**: Sophisticated Excel merge cell expansion
- **Error Recovery**: Robust error handling and data recovery mechanisms

#### **User Interface Features**
- **EJS Template Engine**: Server-side rendering for dynamic content
- **Pagination System**: Efficient handling of large datasets
- **Advanced Search**: Multi-criteria filtering and search
- **Results Display**: Tabular view with sorting and filtering options
- **Client Space**: Dedicated interface for client risk assessment results

#### **Technical Architecture**
- **Express.js Server**: Dedicated server for AML processing
- **File Upload System**: Multer-based file handling with validation
- **Data Storage**: In-memory storage with optional persistence
- **Processing Pipeline**: Multi-stage data processing and validation
- **API Integration**: RESTful API for data access and manipulation

### üìÑ **Doc Secure - Enterprise Document Management**

Doc Secure provides a comprehensive, secure document management system with advanced categorization, search, and security features:

#### **Document Management Features**

**Secure Storage System**
- **SHA256 Hash Verification**: Cryptographic integrity checking
- **Duplicate Detection**: Automatic duplicate file prevention
- **File Type Validation**: Comprehensive MIME type checking
- **Size Limit Enforcement**: Configurable file size restrictions (default: 50MB)
- **Secure File Paths**: Files stored outside web root for security

**Category-Based Organization**
- **Proc√©dures**: Standard operating procedures and workflows
- **Modes d'emploi**: User manuals and instruction guides
- **Notes internes**: Internal communications and memos
- **Politiques**: Corporate policies and governance documents

**Advanced Search & Filtering**
- **Full-Text Search**: Search across document titles and metadata
- **Category Filtering**: Filter by document type and category
- **Date Range Filtering**: Search by upload date or modification date
- **Metadata Search**: Search within document descriptions and tags
- **Sorting Options**: Sort by title, date, size, or relevance

#### **File Format Support**
- **PDF Documents**: `.pdf` files with metadata extraction
- **Microsoft Office**: `.doc`, `.docx`, `.xls`, `.xlsx`, `.ppt`, `.pptx`
- **Text Documents**: Various text formats with encoding detection
- **Archive Files**: Support for compressed file formats

#### **Security & Compliance**
- **Access Control**: Role-based access permissions
- **Audit Trail**: Comprehensive logging of all document operations
- **Version Control**: Document versioning and change tracking
- **Backup System**: Automated backup and recovery procedures
- **Encryption**: At-rest encryption for sensitive documents

#### **Database Schema**
```sql
CREATE TABLE documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,
    original_filename TEXT NOT NULL,
    stored_filename TEXT NOT NULL,
    category TEXT NOT NULL,
    description TEXT,
    file_size INTEGER NOT NULL,
    file_hash TEXT NOT NULL UNIQUE,
    mime_type TEXT,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    file_path TEXT NOT NULL,
    metadata TEXT  -- JSON string for additional metadata
);
```

#### **API Endpoints**
- **Upload**: `POST /api/docsecure/upload` - File upload with metadata
- **List**: `GET /api/docsecure/documents` - Document listing with filters
- **Download**: `GET /api/docsecure/download/[id]` - Secure file download
- **Delete**: `DELETE /api/docsecure/documents/[id]` - Document removal
- **Update**: `PUT /api/docsecure/documents/[id]` - Metadata updates

### üìà **R-Sense - Advanced Risk Calculation & Analytics**

R-Sense is a sophisticated risk calculation and portfolio management system that provides advanced analytics and risk modeling capabilities:

#### **Risk Management Dashboard**

**Key Performance Indicators**
- **Risk Treatment Activities**: 78.0% completion rate tracking
- **Ongoing Activities**: 14 active risk treatment processes
- **Control Implementation**: 50.0% implementation rate monitoring
- **Non-Implemented Controls**: 3 critical controls requiring attention

**Asset Management**
- **Asset Register**: Comprehensive inventory of all organizational assets
- **Asset Classification**: Categorization by type, value, and risk level
- **Asset Tracking**: Real-time monitoring of asset status and location
- **Asset Valuation**: Automated valuation and depreciation calculations

#### **Risk Analysis Tools**

**Risk Matrix Visualization**
- **Probability vs. Impact**: Interactive risk matrix with color coding
- **Risk Categories**: Classification by severity and likelihood
- **Mitigation Strategies**: Linked mitigation plans for each risk
- **Trend Analysis**: Historical risk evolution tracking

**Control Measures Management**
- **Control Effectiveness**: Measurement of control implementation success
- **Control Testing**: Automated and manual control testing procedures
- **Control Gaps**: Identification of control deficiencies
- **Remediation Tracking**: Progress monitoring for control improvements

**Asset Type Distribution**
- **Portfolio Analysis**: Breakdown of assets by type and category
- **Risk Concentration**: Identification of risk concentration areas
- **Diversification Metrics**: Portfolio diversification analysis
- **Performance Tracking**: Asset performance monitoring and reporting

#### **Data Import/Export Capabilities**

**Import Functionality**
- **Bilan**: Balance sheet data import
- **PTF Investissement**: Investment portfolio data
- **PTF Transaction**: Transaction portfolio data
- **PTF Placements**: Placement portfolio data
- **PTF OPCVM Actions**: UCITS equity fund data
- **PTF Titres**: Securities portfolio data
- **ETAT DAT**: Term deposit statements
- **Etat Emprunts**: Loan statements
- **Etat REPO/REVREPO**: Repurchase agreement data
- **Etat Prets**: Lending statements

**Export Options**
- **PDF Reports**: Formatted reports for presentation
- **Excel Spreadsheets**: Data export for analysis
- **CSV Files**: Raw data export for integration

#### **Technical Implementation**
- **React Components**: Modular component architecture
- **Real-time Updates**: Live data refresh and synchronization
- **Responsive Design**: Mobile-optimized interface
- **Chart Libraries**: Advanced visualization with Recharts
- **Data Validation**: Comprehensive input validation and error handling

### üîê **Admin Panel - System Administration**

The Admin Panel provides comprehensive system administration capabilities with secure authentication and management tools:

#### **Authentication System**
- **Multi-layer Authentication**: Session-based with localStorage fallback
- **Role-based Access Control**: Different permission levels for users
- **Secure Login**: Encrypted password handling and session management
- **Auto-logout**: Automatic session expiration for security
- **Audit Logging**: Comprehensive logging of admin activities

#### **System Management**
- **User Management**: Create, modify, and delete user accounts
- **System Monitoring**: Real-time system health and performance metrics
- **Configuration Management**: System-wide configuration settings
- **Backup Management**: Automated backup scheduling and restoration
- **Log Management**: System log viewing and analysis tools

## üõ†Ô∏è Detailed Technical Implementation

### **Frontend Architecture**

#### **Next.js 15 App Router Structure**
```
app/
‚îú‚îÄ‚îÄ layout.tsx                    # Root layout with global styles
‚îú‚îÄ‚îÄ page.tsx                     # Main dashboard landing page
‚îú‚îÄ‚îÄ globals.css                  # Global CSS variables and styles
‚îú‚îÄ‚îÄ admin/
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                # Admin panel with authentication
‚îú‚îÄ‚îÄ amlcenter/
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                # AML Center redirect handler
‚îú‚îÄ‚îÄ docsecure/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                # Doc Secure main redirect
‚îÇ   ‚îú‚îÄ‚îÄ documents/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Document management interface
‚îÇ   ‚îú‚îÄ‚îÄ import/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Document upload interface
‚îÇ   ‚îú‚îÄ‚îÄ dashboard/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Doc Secure dashboard
‚îÇ   ‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Document analytics
‚îÇ   ‚îú‚îÄ‚îÄ settings/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # System settings
‚îÇ   ‚îî‚îÄ‚îÄ admin/
‚îÇ       ‚îî‚îÄ‚îÄ page.tsx            # Admin-specific features
‚îú‚îÄ‚îÄ rep-watch/
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                # Rep Watch redirect to HTML dashboard
‚îú‚îÄ‚îÄ rsense/
‚îÇ   ‚îú‚îÄ‚îÄ page.tsx                # R-Sense main dashboard
‚îÇ   ‚îú‚îÄ‚îÄ layout.tsx              # R-Sense specific layout
‚îÇ   ‚îú‚îÄ‚îÄ analytics/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Risk analytics
‚îÇ   ‚îú‚îÄ‚îÄ reporting/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Risk reporting
‚îÇ   ‚îú‚îÄ‚îÄ risks/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Risk management
‚îÇ   ‚îú‚îÄ‚îÄ settings/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # R-Sense settings
‚îÇ   ‚îú‚îÄ‚îÄ upload/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # Data upload interface
‚îÇ   ‚îú‚îÄ‚îÄ users/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ page.tsx            # User management
‚îÇ   ‚îî‚îÄ‚îÄ notifications/
‚îÇ       ‚îî‚îÄ‚îÄ page.tsx            # Notification center
‚îî‚îÄ‚îÄ api/                        # API routes
    ‚îú‚îÄ‚îÄ health/
    ‚îÇ   ‚îî‚îÄ‚îÄ route.ts            # System health check
    ‚îú‚îÄ‚îÄ test-auth/
    ‚îÇ   ‚îî‚îÄ‚îÄ route.ts            # Authentication testing
    ‚îú‚îÄ‚îÄ admin/
    ‚îÇ   ‚îî‚îÄ‚îÄ auth/               # Admin authentication APIs
    ‚îú‚îÄ‚îÄ aml/                    # AML Center APIs
    ‚îÇ   ‚îú‚îÄ‚îÄ match-excel/
    ‚îÇ   ‚îú‚îÄ‚îÄ process-risk-assessment/
    ‚îÇ   ‚îú‚îÄ‚îÄ proxy/
    ‚îÇ   ‚îú‚îÄ‚îÄ render-page/
    ‚îÇ   ‚îú‚îÄ‚îÄ sanctions/
    ‚îÇ   ‚îú‚îÄ‚îÄ search/
    ‚îÇ   ‚îú‚îÄ‚îÄ upload-pdf/
    ‚îÇ   ‚îî‚îÄ‚îÄ upload-xml/
    ‚îî‚îÄ‚îÄ docsecure/              # Doc Secure APIs
        ‚îú‚îÄ‚îÄ auth/
        ‚îú‚îÄ‚îÄ documents/
        ‚îú‚îÄ‚îÄ download/
        ‚îî‚îÄ‚îÄ upload/
```

#### **Component Architecture**
```
components/
‚îú‚îÄ‚îÄ ui/                         # Radix UI-based components
‚îÇ   ‚îú‚îÄ‚îÄ button.tsx             # Customized button component
‚îÇ   ‚îú‚îÄ‚îÄ card.tsx               # Card layout component
‚îÇ   ‚îú‚îÄ‚îÄ dialog.tsx             # Modal dialog component
‚îÇ   ‚îú‚îÄ‚îÄ input.tsx              # Form input component
‚îÇ   ‚îú‚îÄ‚îÄ table.tsx              # Data table component
‚îÇ   ‚îú‚îÄ‚îÄ tabs.tsx               # Tab navigation component
‚îÇ   ‚îú‚îÄ‚îÄ toast.tsx              # Notification component
‚îÇ   ‚îî‚îÄ‚îÄ [50+ other components] # Complete UI component library
‚îú‚îÄ‚îÄ admin/                      # Admin-specific components
‚îÇ   ‚îú‚îÄ‚îÄ admin-dashboard.tsx    # Main admin interface
‚îÇ   ‚îî‚îÄ‚îÄ admin-login.tsx        # Admin authentication form
‚îú‚îÄ‚îÄ docsecure/                  # Doc Secure components
‚îÇ   ‚îú‚îÄ‚îÄ dashboard-layout.tsx   # Doc Secure layout wrapper
‚îÇ   ‚îú‚îÄ‚îÄ main-nav.tsx           # Navigation component
‚îÇ   ‚îú‚îÄ‚îÄ admin-dashboard.tsx    # Admin features
‚îÇ   ‚îú‚îÄ‚îÄ admin-import-dialog.tsx # Import dialog
‚îÇ   ‚îî‚îÄ‚îÄ admin-login.tsx        # Admin login
‚îú‚îÄ‚îÄ rsense/                     # R-Sense components
‚îÇ   ‚îú‚îÄ‚îÄ asset-register.tsx     # Asset management table
‚îÇ   ‚îî‚îÄ‚îÄ risk-matrix.tsx        # Risk visualization matrix
‚îú‚îÄ‚îÄ mobile-nav.tsx             # Mobile navigation component
‚îî‚îÄ‚îÄ theme-provider.tsx         # Theme context provider
```

### **Backend Architecture**

#### **Integrated Server Configuration**
The application uses a sophisticated integrated server (`integrated-server.js`) that combines multiple technologies:

```javascript
// Server Architecture Overview
const express = require('express');
const next = require('next');
const multer = require('multer');

// Next.js Integration
const nextApp = next({ dev, hostname, port });
const handle = nextApp.getRequestHandler();

// Express.js Middleware Stack
app.use(express.json());                    // JSON body parsing
app.use(express.urlencoded({ extended: true })); // Form data parsing
app.use('/amlcenter', express.static(...)); // Static file serving
app.set('view engine', 'ejs');             // Template engine for AML Center
```

#### **File Processing Pipeline**

**PDF Processing Engine**
```javascript
// Advanced PDF text extraction
const pdfParse = require('pdf-parse');

function processPdfText(text) {
  // Multi-stage parsing pipeline:
  // 1. Entity extraction (KPe, QDe, IQe, CDe)
  // 2. Person extraction (CDi, GBi, QDi, IQi, SOi)
  // 3. Generic pattern matching
  // 4. Duplicate detection and cleanup
  // 5. Data validation and normalization
}
```

**Excel Processing System**
```javascript
// Sophisticated Excel analysis
function processExcelFile(filePath) {
  // Features:
  // - Merged cell expansion
  // - Multi-sheet processing
  // - Risk category extraction
  // - Client information parsing
  // - Data validation and cleanup
}
```

**XML Data Processing**
```javascript
// XML parsing and transformation
const xml2js = require('xml2js');

function processXmlData(result) {
  // Structured data extraction from regulatory XML files
  // Handles complex nested structures and arrays
  // Validates data integrity and format compliance
}
```

#### **Database Architecture**

**SQLite Implementation for Doc Secure**
```python
# Python-based database management
class DocSecureDatabase:
    def __init__(self, db_path="docsecureDOCS/metadata.db"):
        # Database initialization and schema creation
        # Table management and indexing
        # Transaction handling and rollback

    def add_document(self, title, filename, category, description, file_size, file_hash):
        # Secure document insertion with validation
        # Duplicate detection via hash comparison
        # Metadata extraction and storage

    def search_documents(self, query, category=None):
        # Advanced search with full-text capabilities
        # Category filtering and sorting
        # Pagination and result limiting
```

**In-Memory Data Management**
```javascript
// Real-time data storage for AML Center
let pdfData = null;           // Sanctions list data
let clientData = null;        // Client risk assessment data
let clientCount = 0;          // Active client count
const processedNames = new Set(); // Duplicate prevention
```

### **Security Implementation**

#### **Multi-Layer Authentication System**
```javascript
// Admin authentication with fallback
const checkAuthStatus = async () => {
  try {
    // Primary: API-based verification
    const response = await fetch('/api/admin/auth/verify', {
      method: 'GET',
      credentials: 'include',
    });

    // Fallback: localStorage verification
    const sessionData = localStorage.getItem('admin-session');
    const userData = localStorage.getItem('admin-user');

    // Session validation and user state management
  } catch (error) {
    // Error handling and graceful degradation
  }
};
```

#### **File Upload Security**
```javascript
// Comprehensive file validation
const storage = multer.diskStorage({
  destination: function (req, file, cb) {
    // Secure upload directory management
    cb(null, uploadsDir);
  },
  filename: function (req, file, cb) {
    // Unique filename generation with timestamp
    cb(null, file.fieldname + '-' + Date.now() + path.extname(file.originalname));
  }
});

// File type and size validation
const upload = multer({
  storage: storage,
  fileFilter: (req, file, cb) => {
    // MIME type validation
    // File extension verification
    // Security header checking
  },
  limits: {
    fileSize: 50 * 1024 * 1024 // 50MB limit
  }
});
```

#### **Input Sanitization and Validation**
```javascript
// Comprehensive input validation
function sanitizeInput(input) {
  // XSS prevention
  // SQL injection protection
  // Path traversal prevention
  // Special character handling
}
```

### **Performance Optimization**

#### **Frontend Optimization**
- **Server-Side Rendering**: Next.js SSR for fast initial page loads
- **Code Splitting**: Automatic code splitting for optimal bundle sizes
- **Image Optimization**: Next.js Image component with lazy loading
- **CSS Optimization**: Tailwind CSS purging and minification
- **Bundle Analysis**: Webpack bundle analyzer for size optimization

#### **Backend Optimization**
- **Caching Strategy**: In-memory caching for frequently accessed data
- **File Processing**: Streaming file processing for large documents
- **Database Indexing**: Optimized database queries with proper indexing
- **Memory Management**: Efficient memory usage and garbage collection

#### **Network Optimization**
- **Compression**: Gzip compression for all responses
- **CDN Integration**: Static asset delivery optimization
- **HTTP/2 Support**: Modern protocol support for faster loading
- **Caching Headers**: Proper cache control for static resources

## üì¶ Installation and Setup Guide

### **System Requirements**

#### **Minimum Requirements**
- **Operating System**: Windows 10/11, macOS 10.15+, Ubuntu 18.04+
- **Node.js**: Version 18.0.0 or higher
- **RAM**: 4GB minimum, 8GB recommended
- **Storage**: 2GB free space for application and data
- **Browser**: Chrome 90+, Firefox 88+, Safari 14+, Edge 90+

#### **Recommended Requirements**
- **Operating System**: Latest stable versions
- **Node.js**: Version 20.0.0 or higher
- **RAM**: 16GB for optimal performance
- **Storage**: 10GB for extensive document storage
- **Network**: Stable internet connection for updates

### **Installation Methods**

#### **Method 1: Standard Installation (Recommended)**

1. **Clone the Repository**
   ```bash
   git clone https://github.com/gitsupportb/CRC360.git
   cd CRC360
   ```

2. **Install Dependencies**
   ```bash
   # Using npm
   npm install

   # Using pnpm (recommended for faster installation)
   pnpm install

   # Using yarn
   yarn install
   ```

3. **Environment Configuration**
   ```bash
   # Create environment file
   cp .env.example .env.local

   # Edit configuration
   nano .env.local
   ```

4. **Initialize Database**
   ```bash
   # Initialize Doc Secure database
   python scripts/init_docsecure.py

   # Verify database creation
   ls -la docsecureDOCS/
   ```

5. **Start Development Server**
   ```bash
   # Development mode
   npm run dev

   # Production mode
   npm run build
   npm start
   ```

#### **Method 2: Docker Installation**

1. **Using Docker Compose (Recommended)**
   ```bash
   # Clone repository
   git clone https://github.com/gitsupportb/CRC360.git
   cd CRC360

   # Start with Docker Compose
   docker-compose up -d

   # View logs
   docker-compose logs -f
   ```

2. **Manual Docker Build**
   ```bash
   # Build image
   docker build -t crc360:latest .

   # Run container
   docker run -d \
     --name crc360 \
     -p 3000:3000 \
     -v $(pwd)/uploads:/app/uploads \
     -v $(pwd)/docsecureDOCS:/app/docsecureDOCS \
     crc360:latest
   ```

#### **Method 3: Production Deployment**

1. **Server Preparation**
   ```bash
   # Update system
   sudo apt update && sudo apt upgrade -y

   # Install Node.js
   curl -fsSL https://deb.nodesource.com/setup_20.x | sudo -E bash -
   sudo apt-get install -y nodejs

   # Install PM2 for process management
   npm install -g pm2
   ```

2. **Application Deployment**
   ```bash
   # Clone and setup
   git clone https://github.com/gitsupportb/CRC360.git
   cd CRC360
   npm install --production
   npm run build

   # Start with PM2
   pm2 start ecosystem.config.js
   pm2 save
   pm2 startup
   ```

3. **Nginx Configuration**
   ```nginx
   server {
       listen 80;
       server_name your-domain.com;

       location / {
           proxy_pass http://localhost:3000;
           proxy_http_version 1.1;
           proxy_set_header Upgrade $http_upgrade;
           proxy_set_header Connection 'upgrade';
           proxy_set_header Host $host;
           proxy_set_header X-Real-IP $remote_addr;
           proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
           proxy_set_header X-Forwarded-Proto $scheme;
           proxy_cache_bypass $http_upgrade;
       }
   }
   ```

### **Configuration Management**

#### **Environment Variables**
```env
# Application Configuration
NODE_ENV=production
PORT=3000
HOST=localhost

# File Upload Configuration
UPLOAD_MAX_SIZE=52428800          # 50MB in bytes
UPLOAD_ALLOWED_TYPES=pdf,docx,xlsx,pptx,xml

# Database Configuration
DB_PATH=./docsecureDOCS/metadata.db
BACKUP_INTERVAL=24                # Hours

# Security Configuration
SESSION_SECRET=your-secret-key
ADMIN_PASSWORD_HASH=your-hash
JWT_SECRET=your-jwt-secret

# Logging Configuration
LOG_LEVEL=info
LOG_FILE=./logs/application.log

# Performance Configuration
CACHE_TTL=3600                    # Cache time-to-live in seconds
MAX_CONCURRENT_UPLOADS=5
```

#### **Advanced Configuration**
```javascript
// next.config.mjs
/** @type {import('next').NextConfig} */
const nextConfig = {
  experimental: {
    serverComponentsExternalPackages: ['pdf-parse', 'xlsx', 'xml2js']
  },
  webpack: (config, { isServer }) => {
    if (isServer) {
      config.externals.push('pdf-parse');
    }
    return config;
  },
  async headers() {
    return [
      {
        source: '/api/:path*',
        headers: [
          { key: 'Access-Control-Allow-Origin', value: '*' },
          { key: 'Access-Control-Allow-Methods', value: 'GET,POST,PUT,DELETE,OPTIONS' },
          { key: 'Access-Control-Allow-Headers', value: 'Content-Type, Authorization' },
        ],
      },
    ];
  },
};

export default nextConfig;
```

## üöÄ Comprehensive Usage Guide

### **Getting Started - First Time Setup**

#### **Initial System Access**
1. **Navigate to Main Dashboard**
   - Open your web browser
   - Go to `http://localhost:3000` (development) or your configured domain
   - You'll see the CRC360 main dashboard with four module cards

2. **System Health Check**
   - Click on the status indicator in the header (green dot = healthy)
   - Visit `/api/health` to verify all systems are operational
   - Check that all module cards are responsive and clickable

3. **Admin Panel Setup**
   - Navigate to `/admin` for first-time admin configuration
   - Default credentials (change immediately):
     - Username: `admin`
     - Password: `admin123`
   - Configure user accounts and permissions

#### **Module Navigation**
- **Rep Watch**: Click the "Rep Watch" card or navigate to `/rep-watch`
- **AML Center**: Click the "AML CENTER" card or navigate to `/amlcenter`
- **Doc Secure**: Click the "DOC SECURE" card or navigate to `/docsecure`
- **R-Sense**: Click the "R-SENSE" card or navigate to `/rsense`

### **Rep Watch - Detailed Usage Instructions**

#### **Dashboard Overview**
The Rep Watch dashboard provides a comprehensive view of all regulatory reporting requirements and their current status.

**Main Interface Elements:**
- **Calendar Heatmap**: Visual representation of reporting activity
- **Progress Summary**: Overall completion statistics
- **Category Tabs**: Three main reporting categories (I, II, III)
- **Task Management**: Monthly task organization and tracking

#### **Working with Reporting Categories**

**Category I: Situation comptable et √©tats annexes**
1. **Access**: Click on the "Category I" tab
2. **View Reports**: Browse the 26 different report types
3. **Track Progress**: Use checkboxes to mark completed reports
4. **Set Deadlines**: Configure custom deadline reminders
5. **Upload Files**: Use the upload button to submit completed reports

**Category II: Etats de synth√®se et documents compl√©mentaires**
1. **Navigate**: Select "Category II" from the tab menu
2. **Review Requirements**: Examine the 23 report types and their frequencies
3. **Monitor Status**: Check completion status and upcoming deadlines
4. **Submit Reports**: Upload completed documentation

**Category III: Etats relatifs √† la r√©glementation prudentielle**
1. **Open**: Click the "Category III" tab
2. **Manage**: Handle the 19 prudential regulation reports
3. **Track Compliance**: Monitor regulatory compliance status
4. **Generate Reports**: Create and submit required documentation

#### **Monthly Task Management**
1. **Access Monthly View**: Click on "This Month Tasks" tab
2. **Review Tasks**: See all reports due in the current month
3. **Priority Management**: Tasks are color-coded by priority:
   - **Red**: Critical (due within 3 days)
   - **Orange**: High (due within 7 days)
   - **Yellow**: Medium (due within 14 days)
   - **Green**: Low (due within 30 days)
4. **Export for Presentations**: Click "Download Table" to export as PNG

#### **File Upload Process**
1. **Open Upload Modal**: Click the "Upload File" button
2. **Select Category**: Choose the appropriate reporting category
3. **Choose Report**: Select the specific report from the filtered list
4. **Upload File**: Drag and drop or browse for your file
5. **Confirm Upload**: Review details and confirm submission
6. **Track Progress**: Monitor upload progress and completion

#### **Analytics and Reporting**
1. **Progress Overview**: View completion percentages and trends
2. **Upload Statistics**: Monitor file upload activity and success rates
3. **Calendar Heatmap**: Visualize reporting activity over time
4. **Export Data**: Download progress reports in CSV format

### **AML Center - Comprehensive Operations Guide**

#### **Sanctions List Management**

**PDF Upload and Processing**
1. **Access Upload Interface**: Navigate to the AML Center main page
2. **Upload PDF File**:
   - Click "Choose PDF File" button
   - Select your sanctions list PDF file
   - Click "Upload and Process PDF"
3. **Processing Results**:
   - System extracts individual entries (persons and entities)
   - Automatic categorization by type (CDi, QDi, KPe, etc.)
   - Duplicate detection and removal
   - Data validation and cleanup

**XML Data Import**
1. **Select XML Upload**: Choose "Upload XML File" option
2. **File Selection**: Browse and select your XML sanctions file
3. **Processing**: System parses structured XML data
4. **Validation**: Automatic data validation and error checking
5. **Integration**: Data merged with existing sanctions database

**Search and Filtering**
1. **Basic Search**: Use the search bar for name-based queries
2. **Advanced Filters**:
   - **ID Filter**: Search by specific sanctions ID
   - **Name Filter**: Filter by name patterns
   - **Type Filter**: Filter by person/entity type
   - **Nationality Filter**: Filter by nationality
3. **Results Display**: Paginated results with sorting options
4. **Export Results**: Download filtered results for external use

#### **Client Risk Assessment**

**Excel File Processing**
1. **Upload Risk Assessment File**:
   - Navigate to "Client Space" section
   - Click "Upload Risk Assessment Excel"
   - Select your client risk assessment spreadsheet
2. **Automatic Processing**:
   - System extracts client information from multiple sheets
   - Processes risk categories and factors
   - Calculates overall risk scores
   - Generates risk assessment reports

**Risk Category Analysis**
The system analyzes five key risk categories:

1. **Zone g√©ographique (Geographic Zone)**
   - Country risk assessment
   - Sanctions list checking
   - Political stability analysis
   - Economic risk factors

2. **Caract√©ristiques du client (Client Characteristics)**
   - Business type and structure
   - Ownership information
   - Financial profile analysis
   - Industry risk assessment

3. **R√©putation du client (Client Reputation)**
   - Media screening results
   - Adverse news analysis
   - Regulatory actions history
   - Public records review

4. **Nature produits/op√©rations (Product/Operation Nature)**
   - Product complexity assessment
   - Transaction pattern analysis
   - Service risk evaluation
   - Operational risk factors

5. **Canal de distribution (Distribution Channel)**
   - Channel risk assessment
   - Intermediary evaluation
   - Distribution method analysis
   - Third-party risk factors

**Risk Assessment Results**
1. **Client Dashboard**: View all processed clients
2. **Individual Reports**: Detailed risk assessment for each client
3. **Risk Scoring**: Automated risk level calculation (Low, Medium, High)
4. **Recommendations**: System-generated risk mitigation suggestions

#### **Data Matching and Analysis**

**Excel-to-Sanctions Matching**
1. **Upload Client List**: Provide Excel file with client names
2. **Automatic Matching**: System compares against sanctions database
3. **Similarity Analysis**: Uses string similarity algorithms for fuzzy matching
4. **Results Review**: Manual review of potential matches
5. **False Positive Management**: Tools to manage and exclude false positives

### **Doc Secure - Document Management Operations**

#### **Document Upload Process**

**Standard Upload Procedure**
1. **Navigate to Import Page**: Go to `/docsecure/import`
2. **Fill Document Information**:
   - **Title**: Descriptive document title
   - **Category**: Select from predefined categories:
     - Proc√©dures (Procedures)
     - Modes d'emploi (User Manuals)
     - Notes internes (Internal Notes)
     - Politiques (Policies)
   - **Description**: Optional detailed description
3. **File Selection**:
   - Click "Choose File" or drag and drop
   - Supported formats: PDF, DOCX, XLSX, PPTX, DOC, XLS, PPT
   - Maximum file size: 50MB
4. **Upload Execution**:
   - Click "Importer le document"
   - Monitor upload progress
   - Receive confirmation upon completion

**Bulk Upload Process**
1. **Prepare Files**: Organize files in appropriate folders
2. **Batch Selection**: Select multiple files for upload
3. **Category Assignment**: Assign categories to multiple files
4. **Bulk Processing**: System processes all files simultaneously
5. **Results Review**: Review upload results and handle any errors

#### **Document Management Interface**

**Document Browser**
1. **Access Documents**: Navigate to `/docsecure/documents`
2. **View Options**:
   - **List View**: Detailed table with metadata
   - **Grid View**: Thumbnail-based layout
   - **Category Tabs**: Filter by document category
3. **Search Functionality**:
   - **Quick Search**: Search bar for title and content
   - **Advanced Search**: Multi-criteria search options
   - **Filter Options**: Date range, file type, size filters

**Document Operations**
1. **Preview**: Click the eye icon to preview document details
2. **Download**: Click download icon to get original file
3. **Edit Metadata**: Modify title, description, and category
4. **Delete**: Remove document and associated file
5. **Share**: Generate secure sharing links (if enabled)

#### **Advanced Features**

**Document Analytics**
1. **Usage Statistics**: Track document access and downloads
2. **Storage Analytics**: Monitor storage usage by category
3. **User Activity**: Track user interactions with documents
4. **Trend Analysis**: Identify popular documents and usage patterns

**Security Features**
1. **Access Control**: Role-based document access
2. **Audit Trail**: Complete log of all document operations
3. **Version Control**: Track document versions and changes
4. **Backup System**: Automated backup and recovery

### **R-Sense - Risk Analytics and Calculation**

#### **Dashboard Navigation**

**Main Dashboard Overview**
1. **Key Metrics Cards**: Four primary KPI indicators
   - Risk Treatment Activities Completed (78.0%)
   - Ongoing Risk Treatment Activities (14)
   - Controls Implemented (50.0%)
   - Controls Not Implemented (3)
2. **Asset Register**: Comprehensive asset inventory
3. **Risk Matrix**: Visual risk assessment grid
4. **Control Measures**: Implementation status tracking
5. **Asset Distribution**: Portfolio breakdown analysis

#### **Asset Management**

**Asset Register Operations**
1. **View Assets**: Browse complete asset inventory
2. **Add New Assets**: Register new organizational assets
3. **Update Information**: Modify asset details and valuations
4. **Risk Assessment**: Assign risk levels to assets
5. **Categorization**: Organize assets by type and importance

**Asset Type Distribution Analysis**
1. **Portfolio Overview**: Visual breakdown of asset types
2. **Risk Concentration**: Identify areas of risk concentration
3. **Diversification Analysis**: Assess portfolio diversification
4. **Performance Tracking**: Monitor asset performance over time

#### **Risk Assessment Tools**

**Risk Matrix Management**
1. **Risk Plotting**: Place risks on probability vs. impact matrix
2. **Risk Categories**: Classify risks by severity levels
3. **Mitigation Planning**: Link mitigation strategies to risks
4. **Trend Monitoring**: Track risk evolution over time

**Control Measures Tracking**
1. **Control Inventory**: Maintain list of all control measures
2. **Implementation Status**: Track control implementation progress
3. **Effectiveness Testing**: Monitor control effectiveness
4. **Gap Analysis**: Identify control deficiencies

#### **Data Import and Export**

**Import Functionality**
1. **Access Import Tool**: Click the "IMPORT" button
2. **Select Data Type**: Choose from available import types:
   - Bilan (Balance Sheet)
   - PTF Investissement (Investment Portfolio)
   - PTF Transaction (Transaction Portfolio)
   - PTF Placements (Placement Portfolio)
   - PTF OPCVM Actions (UCITS Equity Funds)
   - PTF Titres (Securities Portfolio)
   - ETAT DAT (Term Deposit Statements)
   - Etat Emprunts (Loan Statements)
   - Etat REPO/REVREPO (Repo Agreements)
   - Etat Prets (Lending Statements)
3. **Date Selection**: Specify the data date
4. **File Upload**: Select and upload the data file
5. **Processing**: System processes and integrates the data

**Export Options**
1. **Format Selection**: Choose export format:
   - PDF: Formatted reports for presentation
   - XLSX: Spreadsheet format for analysis
   - CSV: Raw data for integration
2. **Data Range**: Select date range for export
3. **Content Selection**: Choose specific data sets to export
4. **Download**: Generate and download the export file

### **Admin Panel - System Administration**

#### **User Management**
1. **Access Admin Panel**: Navigate to `/admin`
2. **User Accounts**: Create, modify, and delete user accounts
3. **Role Assignment**: Assign roles and permissions
4. **Access Control**: Configure module-specific access rights
5. **Session Management**: Monitor and manage user sessions

#### **System Configuration**
1. **Global Settings**: Configure system-wide parameters
2. **Module Settings**: Adjust settings for individual modules
3. **Security Configuration**: Set security policies and parameters
4. **Backup Configuration**: Configure automated backup schedules
5. **Performance Tuning**: Optimize system performance settings

#### **Monitoring and Maintenance**
1. **System Health**: Monitor system performance and health
2. **Log Management**: View and analyze system logs
3. **Error Tracking**: Monitor and resolve system errors
4. **Update Management**: Manage system updates and patches
5. **Database Maintenance**: Perform database optimization tasks

## üîß Advanced Configuration and Customization

### **System Configuration Files**

#### **Next.js Configuration (`next.config.mjs`)**
```javascript
/** @type {import('next').NextConfig} */
const nextConfig = {
  // Experimental features for enhanced performance
  experimental: {
    serverComponentsExternalPackages: ['pdf-parse', 'xlsx', 'xml2js'],
    optimizeCss: true,
    optimizePackageImports: ['lucide-react', '@radix-ui/react-icons']
  },

  // Webpack configuration for external packages
  webpack: (config, { isServer, dev }) => {
    if (isServer) {
      config.externals.push('pdf-parse', 'canvas', 'jsdom');
    }

    // Optimize bundle size
    if (!dev) {
      config.optimization.splitChunks.chunks = 'all';
    }

    return config;
  },

  // Security headers
  async headers() {
    return [
      {
        source: '/(.*)',
        headers: [
          { key: 'X-Frame-Options', value: 'DENY' },
          { key: 'X-Content-Type-Options', value: 'nosniff' },
          { key: 'Referrer-Policy', value: 'strict-origin-when-cross-origin' },
          { key: 'Permissions-Policy', value: 'camera=(), microphone=(), geolocation=()' }
        ],
      },
      {
        source: '/api/:path*',
        headers: [
          { key: 'Access-Control-Allow-Origin', value: process.env.ALLOWED_ORIGINS || '*' },
          { key: 'Access-Control-Allow-Methods', value: 'GET,POST,PUT,DELETE,OPTIONS' },
          { key: 'Access-Control-Allow-Headers', value: 'Content-Type, Authorization, X-Requested-With' },
        ],
      },
    ];
  },

  // Image optimization
  images: {
    domains: ['localhost'],
    formats: ['image/webp', 'image/avif'],
    minimumCacheTTL: 60,
  },

  // Compression and optimization
  compress: true,
  poweredByHeader: false,
  generateEtags: false,

  // Environment-specific redirects
  async redirects() {
    return [
      {
        source: '/dashboard',
        destination: '/',
        permanent: true,
      },
    ];
  },
};

export default nextConfig;
```

#### **Tailwind CSS Configuration (`tailwind.config.ts`)**
```typescript
import type { Config } from "tailwindcss"

const config: Config = {
  darkMode: ["class"],
  content: [
    './pages/**/*.{ts,tsx}',
    './components/**/*.{ts,tsx}',
    './app/**/*.{ts,tsx}',
    './src/**/*.{ts,tsx}',
  ],
  theme: {
    container: {
      center: true,
      padding: "2rem",
      screens: {
        "2xl": "1400px",
      },
    },
    extend: {
      colors: {
        // BCP Securities Services Brand Colors
        primary: {
          50: '#fff8f5',
          100: '#ffe4d6',
          200: '#ffb366',
          300: '#ff8c42',
          400: '#ff6b35',
          500: '#e55a2b',
          600: '#cc4a1f',
          700: '#b33a13',
          800: '#992a07',
          900: '#801a00',
        },
        // Custom color palette for modules
        'rep-watch': '#2563eb',
        'aml-center': '#059669',
        'doc-secure': '#7c3aed',
        'r-sense': '#dc2626',
        // UI Colors
        border: "hsl(var(--border))",
        input: "hsl(var(--input))",
        ring: "hsl(var(--ring))",
        background: "hsl(var(--background))",
        foreground: "hsl(var(--foreground))",
        secondary: {
          DEFAULT: "hsl(var(--secondary))",
          foreground: "hsl(var(--secondary-foreground))",
        },
        destructive: {
          DEFAULT: "hsl(var(--destructive))",
          foreground: "hsl(var(--destructive-foreground))",
        },
        muted: {
          DEFAULT: "hsl(var(--muted))",
          foreground: "hsl(var(--muted-foreground))",
        },
        accent: {
          DEFAULT: "hsl(var(--accent))",
          foreground: "hsl(var(--accent-foreground))",
        },
        popover: {
          DEFAULT: "hsl(var(--popover))",
          foreground: "hsl(var(--popover-foreground))",
        },
        card: {
          DEFAULT: "hsl(var(--card))",
          foreground: "hsl(var(--card-foreground))",
        },
      },
      borderRadius: {
        lg: "var(--radius)",
        md: "calc(var(--radius) - 2px)",
        sm: "calc(var(--radius) - 4px)",
      },
      keyframes: {
        "accordion-down": {
          from: { height: "0" },
          to: { height: "var(--radix-accordion-content-height)" },
        },
        "accordion-up": {
          from: { height: "var(--radix-accordion-content-height)" },
          to: { height: "0" },
        },
        "fade-in": {
          from: { opacity: "0", transform: "translateY(10px)" },
          to: { opacity: "1", transform: "translateY(0)" },
        },
        "slide-in": {
          from: { transform: "translateX(-100%)" },
          to: { transform: "translateX(0)" },
        },
      },
      animation: {
        "accordion-down": "accordion-down 0.2s ease-out",
        "accordion-up": "accordion-up 0.2s ease-out",
        "fade-in": "fade-in 0.3s ease-out",
        "slide-in": "slide-in 0.3s ease-out",
      },
      fontFamily: {
        sans: ['Inter', 'system-ui', 'sans-serif'],
        mono: ['JetBrains Mono', 'monospace'],
      },
      spacing: {
        '18': '4.5rem',
        '88': '22rem',
      },
    },
  },
  plugins: [require("tailwindcss-animate")],
}

export default config
```

#### **TypeScript Configuration (`tsconfig.json`)**
```json
{
  "compilerOptions": {
    "lib": ["dom", "dom.iterable", "es6"],
    "allowJs": true,
    "skipLibCheck": true,
    "strict": true,
    "noEmit": true,
    "esModuleInterop": true,
    "module": "esnext",
    "moduleResolution": "bundler",
    "resolveJsonModule": true,
    "isolatedModules": true,
    "jsx": "preserve",
    "incremental": true,
    "plugins": [
      {
        "name": "next"
      }
    ],
    "baseUrl": ".",
    "paths": {
      "@/*": ["./*"],
      "@/components/*": ["./components/*"],
      "@/lib/*": ["./lib/*"],
      "@/utils/*": ["./utils/*"],
      "@/types/*": ["./types/*"],
      "@/hooks/*": ["./hooks/*"],
      "@/styles/*": ["./styles/*"]
    },
    "target": "es2017",
    "forceConsistentCasingInFileNames": true,
    "noUnusedLocals": true,
    "noUnusedParameters": true,
    "noImplicitReturns": true,
    "noFallthroughCasesInSwitch": true
  },
  "include": [
    "next-env.d.ts",
    "**/*.ts",
    "**/*.tsx",
    ".next/types/**/*.ts"
  ],
  "exclude": [
    "node_modules",
    ".next",
    "out",
    "dist"
  ]
}
```

### **Database Configuration and Management**

#### **Doc Secure Database Schema**
```sql
-- Complete database schema for document management
CREATE TABLE IF NOT EXISTS documents (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    title TEXT NOT NULL,
    original_filename TEXT NOT NULL,
    stored_filename TEXT NOT NULL UNIQUE,
    category TEXT NOT NULL CHECK (category IN ('Proc√©dures', 'Modes d''emploi', 'Notes internes', 'Politiques')),
    description TEXT,
    file_size INTEGER NOT NULL CHECK (file_size > 0),
    file_hash TEXT NOT NULL UNIQUE,
    mime_type TEXT NOT NULL,
    upload_date TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    last_modified TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    file_path TEXT NOT NULL,
    metadata TEXT,  -- JSON string for additional metadata
    created_by TEXT DEFAULT 'system',
    updated_by TEXT DEFAULT 'system',
    version INTEGER DEFAULT 1,
    is_active BOOLEAN DEFAULT 1,
    tags TEXT,  -- Comma-separated tags
    access_level TEXT DEFAULT 'public' CHECK (access_level IN ('public', 'restricted', 'confidential')),
    download_count INTEGER DEFAULT 0,
    last_accessed TIMESTAMP
);

-- Indexes for performance optimization
CREATE INDEX IF NOT EXISTS idx_documents_category ON documents(category);
CREATE INDEX IF NOT EXISTS idx_documents_upload_date ON documents(upload_date);
CREATE INDEX IF NOT EXISTS idx_documents_file_hash ON documents(file_hash);
CREATE INDEX IF NOT EXISTS idx_documents_title ON documents(title);
CREATE INDEX IF NOT EXISTS idx_documents_active ON documents(is_active);

-- Full-text search support
CREATE VIRTUAL TABLE IF NOT EXISTS documents_fts USING fts5(
    title,
    description,
    original_filename,
    tags,
    content='documents',
    content_rowid='id'
);

-- Triggers for maintaining FTS index
CREATE TRIGGER IF NOT EXISTS documents_ai AFTER INSERT ON documents BEGIN
    INSERT INTO documents_fts(rowid, title, description, original_filename, tags)
    VALUES (new.id, new.title, new.description, new.original_filename, new.tags);
END;

CREATE TRIGGER IF NOT EXISTS documents_ad AFTER DELETE ON documents BEGIN
    INSERT INTO documents_fts(documents_fts, rowid, title, description, original_filename, tags)
    VALUES('delete', old.id, old.title, old.description, old.original_filename, old.tags);
END;

CREATE TRIGGER IF NOT EXISTS documents_au AFTER UPDATE ON documents BEGIN
    INSERT INTO documents_fts(documents_fts, rowid, title, description, original_filename, tags)
    VALUES('delete', old.id, old.title, old.description, old.original_filename, old.tags);
    INSERT INTO documents_fts(rowid, title, description, original_filename, tags)
    VALUES (new.id, new.title, new.description, new.original_filename, new.tags);
END;

-- Audit table for tracking changes
CREATE TABLE IF NOT EXISTS document_audit (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    document_id INTEGER NOT NULL,
    action TEXT NOT NULL CHECK (action IN ('CREATE', 'UPDATE', 'DELETE', 'DOWNLOAD', 'VIEW')),
    user_id TEXT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    old_values TEXT,  -- JSON string of old values
    new_values TEXT,  -- JSON string of new values
    ip_address TEXT,
    user_agent TEXT,
    FOREIGN KEY (document_id) REFERENCES documents(id)
);

CREATE INDEX IF NOT EXISTS idx_audit_document_id ON document_audit(document_id);
CREATE INDEX IF NOT EXISTS idx_audit_timestamp ON document_audit(timestamp);
CREATE INDEX IF NOT EXISTS idx_audit_action ON document_audit(action);
```

#### **Database Initialization Script**
```python
# scripts/init_docsecure.py
import sqlite3
import os
import json
from datetime import datetime
import hashlib

class DocSecureDatabase:
    def __init__(self, db_path="docsecureDOCS/metadata.db"):
        self.db_path = db_path
        self.base_dir = os.path.dirname(db_path)
        self.categories = {
            'Proc√©dures': 'procedures',
            'Modes d\'emploi': 'modes_emploi',
            'Notes internes': 'notes_internes',
            'Politiques': 'politiques'
        }
        self.allowed_extensions = {
            '.pdf', '.doc', '.docx', '.xls', '.xlsx',
            '.ppt', '.pptx', '.txt', '.rtf'
        }
        self.max_file_size = 50 * 1024 * 1024  # 50MB

        self.init_database()
        self.create_directories()

    def init_database(self):
        """Initialize the database with all required tables and indexes"""
        os.makedirs(self.base_dir, exist_ok=True)

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            # Read and execute schema
            schema_file = os.path.join(os.path.dirname(__file__), 'schema.sql')
            if os.path.exists(schema_file):
                with open(schema_file, 'r') as f:
                    schema = f.read()
                cursor.executescript(schema)
            else:
                # Fallback to embedded schema
                self._create_tables(cursor)

            conn.commit()
            print(f"‚úÖ Database initialized at {self.db_path}")

    def create_directories(self):
        """Create all required directories for document storage"""
        for category, folder in self.categories.items():
            folder_path = os.path.join(self.base_dir, folder)
            os.makedirs(folder_path, exist_ok=True)
            print(f"‚úÖ Created directory: {folder_path}")

    def _create_tables(self, cursor):
        """Create database tables if schema file is not available"""
        # Implementation of table creation
        # (Schema SQL would be embedded here)
        pass

    def add_sample_data(self):
        """Add sample documents for testing"""
        sample_docs = [
            {
                'title': 'Proc√©dure de gestion des risques',
                'category': 'Proc√©dures',
                'description': 'Document d√©crivant les proc√©dures de gestion des risques op√©rationnels'
            },
            {
                'title': 'Guide utilisateur AML Center',
                'category': 'Modes d\'emploi',
                'description': 'Manuel d\'utilisation du module AML Center'
            },
            {
                'title': 'Note interne - Mise √† jour r√©glementaire',
                'category': 'Notes internes',
                'description': 'Information sur les derni√®res mises √† jour r√©glementaires'
            },
            {
                'title': 'Politique de s√©curit√© informatique',
                'category': 'Politiques',
                'description': 'Politique g√©n√©rale de s√©curit√© des syst√®mes d\'information'
            }
        ]

        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.cursor()

            for doc in sample_docs:
                # Create sample file
                filename = f"sample_{doc['title'].lower().replace(' ', '_')}.pdf"
                file_path = os.path.join(
                    self.base_dir,
                    self.categories[doc['category']],
                    filename
                )

                # Create empty file for demonstration
                with open(file_path, 'w') as f:
                    f.write(f"Sample document: {doc['title']}")

                # Calculate file hash
                with open(file_path, 'rb') as f:
                    file_hash = hashlib.sha256(f.read()).hexdigest()

                # Insert into database
                cursor.execute("""
                    INSERT INTO documents (
                        title, original_filename, stored_filename, category,
                        description, file_size, file_hash, mime_type, file_path
                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    doc['title'], filename, filename, doc['category'],
                    doc['description'], os.path.getsize(file_path),
                    file_hash, 'application/pdf', file_path
                ))

            conn.commit()
            print(f"‚úÖ Added {len(sample_docs)} sample documents")

if __name__ == "__main__":
    # Initialize the database
    db = DocSecureDatabase()

    # Add sample data for testing
    db.add_sample_data()

    print("\nüéâ Doc Secure database initialization complete!")
    print(f"üìÅ Database location: {db.db_path}")
    print(f"üìÇ Document storage: {db.base_dir}")
```

### **Performance Optimization Configuration**

#### **PM2 Ecosystem Configuration (`ecosystem.config.js`)**
```javascript
module.exports = {
  apps: [{
    name: 'crc360',
    script: 'integrated-server.js',
    instances: 'max',
    exec_mode: 'cluster',
    env: {
      NODE_ENV: 'development',
      PORT: 3000
    },
    env_production: {
      NODE_ENV: 'production',
      PORT: 3000
    },
    // Performance monitoring
    monitoring: true,
    pmx: true,

    // Memory and CPU limits
    max_memory_restart: '1G',
    min_uptime: '10s',
    max_restarts: 10,

    // Logging configuration
    log_file: './logs/combined.log',
    out_file: './logs/out.log',
    error_file: './logs/error.log',
    log_date_format: 'YYYY-MM-DD HH:mm:ss Z',

    // Advanced options
    watch: false,
    ignore_watch: ['node_modules', 'logs', 'uploads', 'docsecureDOCS'],
    watch_options: {
      followSymlinks: false
    },

    // Graceful shutdown
    kill_timeout: 5000,
    listen_timeout: 3000,

    // Health check
    health_check_grace_period: 3000,

    // Environment-specific settings
    node_args: '--max-old-space-size=2048',

    // Cron restart (daily at 2 AM)
    cron_restart: '0 2 * * *',

    // Merge logs
    merge_logs: true,

    // Time zone
    time: true
  }],

  deploy: {
    production: {
      user: 'deploy',
      host: 'your-server.com',
      ref: 'origin/main',
      repo: 'git@github.com:gitsupportb/CRC360.git',
      path: '/var/www/crc360',
      'post-deploy': 'npm install && npm run build && pm2 reload ecosystem.config.js --env production'
    }
  }
};
```

#### **Nginx Configuration for Production**
```nginx
# /etc/nginx/sites-available/crc360
upstream crc360_backend {
    least_conn;
    server 127.0.0.1:3000 max_fails=3 fail_timeout=30s;
    server 127.0.0.1:3001 max_fails=3 fail_timeout=30s backup;
}

# Rate limiting
limit_req_zone $binary_remote_addr zone=api:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=upload:10m rate=2r/s;

# File upload size limit
client_max_body_size 50M;

server {
    listen 80;
    listen [::]:80;
    server_name crc360.bcpsecurities.ma www.crc360.bcpsecurities.ma;

    # Redirect HTTP to HTTPS
    return 301 https://$server_name$request_uri;
}

server {
    listen 443 ssl http2;
    listen [::]:443 ssl http2;
    server_name crc360.bcpsecurities.ma www.crc360.bcpsecurities.ma;

    # SSL Configuration
    ssl_certificate /etc/letsencrypt/live/crc360.bcpsecurities.ma/fullchain.pem;
    ssl_certificate_key /etc/letsencrypt/live/crc360.bcpsecurities.ma/privkey.pem;
    ssl_session_timeout 1d;
    ssl_session_cache shared:SSL:50m;
    ssl_session_tickets off;

    # Modern configuration
    ssl_protocols TLSv1.2 TLSv1.3;
    ssl_ciphers ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES256-GCM-SHA384:ECDHE-RSA-AES256-GCM-SHA384;
    ssl_prefer_server_ciphers off;

    # HSTS
    add_header Strict-Transport-Security "max-age=63072000" always;

    # Security headers
    add_header X-Frame-Options DENY always;
    add_header X-Content-Type-Options nosniff always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;
    add_header Content-Security-Policy "default-src 'self'; script-src 'self' 'unsafe-inline' 'unsafe-eval' https://cdn.plot.ly https://html2canvas.hertzen.com; style-src 'self' 'unsafe-inline'; img-src 'self' data: https:; font-src 'self' data:; connect-src 'self'; frame-ancestors 'none';" always;

    # Logging
    access_log /var/log/nginx/crc360_access.log;
    error_log /var/log/nginx/crc360_error.log;

    # Gzip compression
    gzip on;
    gzip_vary on;
    gzip_min_length 1024;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_types
        text/plain
        text/css
        text/xml
        text/javascript
        application/json
        application/javascript
        application/xml+rss
        application/atom+xml
        image/svg+xml;

    # Static file caching
    location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg|woff|woff2|ttf|eot)$ {
        expires 1y;
        add_header Cache-Control "public, immutable";
        add_header X-Content-Type-Options nosniff;
        try_files $uri @proxy;
    }

    # API rate limiting
    location /api/ {
        limit_req zone=api burst=20 nodelay;
        limit_req_status 429;
        try_files $uri @proxy;
    }

    # Upload endpoints with special rate limiting
    location ~* /api/.*/upload {
        limit_req zone=upload burst=5 nodelay;
        limit_req_status 429;
        client_body_timeout 300s;
        client_header_timeout 300s;
        proxy_read_timeout 300s;
        proxy_send_timeout 300s;
        try_files $uri @proxy;
    }

    # Main proxy configuration
    location @proxy {
        proxy_pass http://crc360_backend;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection 'upgrade';
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_cache_bypass $http_upgrade;

        # Timeouts
        proxy_connect_timeout 60s;
        proxy_send_timeout 60s;
        proxy_read_timeout 60s;

        # Buffer settings
        proxy_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }

    # Default location
    location / {
        try_files $uri @proxy;
    }

    # Health check endpoint
    location /health {
        access_log off;
        try_files $uri @proxy;
    }

    # Deny access to sensitive files
    location ~ /\. {
        deny all;
        access_log off;
        log_not_found off;
    }

    location ~ /(uploads|docsecureDOCS)/ {
        deny all;
        access_log off;
        log_not_found off;
    }
}
```

## üîí Security and Compliance Framework

### **Multi-Layer Security Architecture**

#### **Authentication and Authorization**

**Session-Based Authentication**
```javascript
// Hybrid authentication system with multiple fallbacks
const authenticationFlow = {
  primary: 'API-based verification with JWT tokens',
  fallback: 'localStorage session management',
  backup: 'Server-side session storage',

  // Authentication verification
  async verifyAuth() {
    try {
      // Primary: API verification
      const response = await fetch('/api/admin/auth/verify', {
        method: 'GET',
        credentials: 'include',
        headers: {
          'Authorization': `Bearer ${this.getToken()}`,
          'X-Requested-With': 'XMLHttpRequest'
        }
      });

      if (response.ok) {
        const result = await response.json();
        return result.authenticated;
      }

      // Fallback: localStorage verification
      return this.verifyLocalStorage();
    } catch (error) {
      console.error('Authentication verification failed:', error);
      return false;
    }
  },

  // Role-based access control
  checkPermissions(requiredRole, userRole) {
    const roleHierarchy = {
      'super_admin': 4,
      'admin': 3,
      'manager': 2,
      'user': 1,
      'guest': 0
    };

    return roleHierarchy[userRole] >= roleHierarchy[requiredRole];
  }
};
```

**Password Security Standards**
- **Minimum Length**: 12 characters
- **Complexity Requirements**:
  - At least one uppercase letter
  - At least one lowercase letter
  - At least one number
  - At least one special character
- **Password Hashing**: bcrypt with salt rounds ‚â• 12
- **Password History**: Prevent reuse of last 12 passwords
- **Account Lockout**: 5 failed attempts trigger 15-minute lockout
- **Session Timeout**: 30 minutes of inactivity

#### **File Upload Security**

**Comprehensive File Validation**
```javascript
// Multi-stage file validation system
const fileSecurityValidator = {
  // MIME type validation
  validateMimeType(file) {
    const allowedTypes = {
      'application/pdf': ['.pdf'],
      'application/vnd.openxmlformats-officedocument.wordprocessingml.document': ['.docx'],
      'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet': ['.xlsx'],
      'application/vnd.openxmlformats-officedocument.presentationml.presentation': ['.pptx'],
      'application/msword': ['.doc'],
      'application/vnd.ms-excel': ['.xls'],
      'application/vnd.ms-powerpoint': ['.ppt'],
      'text/xml': ['.xml'],
      'application/xml': ['.xml']
    };

    return allowedTypes.hasOwnProperty(file.type);
  },

  // File signature validation (magic numbers)
  async validateFileSignature(file) {
    const signatures = {
      'PDF': [0x25, 0x50, 0x44, 0x46],  // %PDF
      'DOCX': [0x50, 0x4B, 0x03, 0x04], // ZIP signature (DOCX is ZIP-based)
      'XLS': [0xD0, 0xCF, 0x11, 0xE0],  // OLE2 signature
      'XML': [0x3C, 0x3F, 0x78, 0x6D]   // <?xm
    };

    const buffer = await file.slice(0, 4).arrayBuffer();
    const bytes = new Uint8Array(buffer);

    // Check against known signatures
    for (const [type, signature] of Object.entries(signatures)) {
      if (signature.every((byte, index) => bytes[index] === byte)) {
        return true;
      }
    }

    return false;
  },

  // Malware scanning simulation
  async scanForMalware(file) {
    // In production, integrate with actual antivirus API
    const suspiciousPatterns = [
      /eval\s*\(/gi,
      /<script[^>]*>/gi,
      /javascript:/gi,
      /vbscript:/gi,
      /onload\s*=/gi,
      /onerror\s*=/gi
    ];

    if (file.type.includes('text') || file.name.endsWith('.xml')) {
      const text = await file.text();
      return !suspiciousPatterns.some(pattern => pattern.test(text));
    }

    return true; // Binary files pass by default
  },

  // Complete validation pipeline
  async validateFile(file) {
    const validations = [
      this.validateMimeType(file),
      await this.validateFileSignature(file),
      await this.scanForMalware(file),
      file.size <= 50 * 1024 * 1024, // 50MB limit
      file.name.length <= 255 // Filename length limit
    ];

    return validations.every(result => result === true);
  }
};
```

**Secure File Storage**
```python
# Python-based secure file storage system
import hashlib
import os
import secrets
from pathlib import Path

class SecureFileStorage:
    def __init__(self, base_path="docsecureDOCS"):
        self.base_path = Path(base_path)
        self.quarantine_path = self.base_path / "quarantine"
        self.ensure_directories()

    def ensure_directories(self):
        """Create secure directory structure"""
        directories = [
            self.base_path,
            self.quarantine_path,
            self.base_path / "procedures",
            self.base_path / "modes_emploi",
            self.base_path / "notes_internes",
            self.base_path / "politiques"
        ]

        for directory in directories:
            directory.mkdir(mode=0o750, parents=True, exist_ok=True)

    def generate_secure_filename(self, original_filename):
        """Generate cryptographically secure filename"""
        # Extract file extension
        extension = Path(original_filename).suffix.lower()

        # Generate random filename
        random_name = secrets.token_urlsafe(32)

        # Combine with timestamp for uniqueness
        timestamp = int(time.time())

        return f"{random_name}_{timestamp}{extension}"

    def calculate_file_hash(self, file_path):
        """Calculate SHA-256 hash of file"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                sha256_hash.update(chunk)
        return sha256_hash.hexdigest()

    def store_file_securely(self, file_data, category, original_filename):
        """Store file with security checks"""
        try:
            # Generate secure filename
            secure_filename = self.generate_secure_filename(original_filename)

            # Determine storage path
            category_map = {
                'Proc√©dures': 'procedures',
                'Modes d\'emploi': 'modes_emploi',
                'Notes internes': 'notes_internes',
                'Politiques': 'politiques'
            }

            storage_dir = self.base_path / category_map.get(category, 'procedures')
            file_path = storage_dir / secure_filename

            # Write file with restricted permissions
            with open(file_path, 'wb') as f:
                f.write(file_data)

            # Set secure file permissions (owner read/write only)
            os.chmod(file_path, 0o600)

            # Calculate and verify file hash
            file_hash = self.calculate_file_hash(file_path)

            return {
                'stored_filename': secure_filename,
                'file_path': str(file_path),
                'file_hash': file_hash,
                'file_size': len(file_data)
            }

        except Exception as e:
            # Move to quarantine on error
            self.quarantine_file(file_data, original_filename, str(e))
            raise SecurityError(f"File storage failed: {e}")

    def quarantine_file(self, file_data, filename, reason):
        """Quarantine suspicious files"""
        quarantine_filename = f"quarantine_{int(time.time())}_{filename}"
        quarantine_path = self.quarantine_path / quarantine_filename

        with open(quarantine_path, 'wb') as f:
            f.write(file_data)

        # Log quarantine event
        self.log_security_event('FILE_QUARANTINED', {
            'filename': filename,
            'reason': reason,
            'quarantine_path': str(quarantine_path)
        })
```

#### **Data Protection and Privacy**

**Encryption Standards**
- **Data at Rest**: AES-256 encryption for sensitive documents
- **Data in Transit**: TLS 1.3 for all communications
- **Database Encryption**: SQLite encryption for metadata
- **Key Management**: Hardware Security Module (HSM) integration
- **Backup Encryption**: Encrypted backups with separate key storage

**Privacy Compliance (GDPR/CCPA)**
```javascript
// Privacy compliance framework
const privacyCompliance = {
  // Data minimization
  collectOnlyNecessaryData(userData) {
    const necessaryFields = [
      'username', 'email', 'role', 'last_login', 'preferences'
    ];

    return Object.keys(userData)
      .filter(key => necessaryFields.includes(key))
      .reduce((obj, key) => {
        obj[key] = userData[key];
        return obj;
      }, {});
  },

  // Data retention policies
  dataRetentionPolicy: {
    'user_sessions': '30 days',
    'audit_logs': '7 years',
    'document_metadata': 'indefinite',
    'uploaded_files': 'user-defined',
    'system_logs': '1 year'
  },

  // Right to be forgotten
  async deleteUserData(userId) {
    const deletionTasks = [
      this.deleteUserSessions(userId),
      this.anonymizeAuditLogs(userId),
      this.deleteUserPreferences(userId),
      this.notifyDataProcessors(userId)
    ];

    await Promise.all(deletionTasks);

    return {
      status: 'completed',
      timestamp: new Date().toISOString(),
      deletedRecords: deletionTasks.length
    };
  },

  // Data portability
  async exportUserData(userId) {
    const userData = await this.collectUserData(userId);

    return {
      format: 'JSON',
      data: userData,
      exportDate: new Date().toISOString(),
      dataTypes: Object.keys(userData)
    };
  }
};
```

### **Audit and Compliance Monitoring**

#### **Comprehensive Audit Trail**
```sql
-- Audit logging system
CREATE TABLE audit_logs (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    user_id TEXT,
    session_id TEXT,
    action TEXT NOT NULL,
    resource_type TEXT NOT NULL,
    resource_id TEXT,
    ip_address TEXT,
    user_agent TEXT,
    request_method TEXT,
    request_url TEXT,
    response_status INTEGER,
    old_values TEXT, -- JSON
    new_values TEXT, -- JSON
    risk_level TEXT CHECK (risk_level IN ('LOW', 'MEDIUM', 'HIGH', 'CRITICAL')),
    compliance_flags TEXT, -- JSON array of compliance requirements
    geolocation TEXT,
    device_fingerprint TEXT,
    success BOOLEAN DEFAULT 1,
    error_message TEXT,
    processing_time_ms INTEGER
);

-- Indexes for performance
CREATE INDEX idx_audit_timestamp ON audit_logs(timestamp);
CREATE INDEX idx_audit_user_id ON audit_logs(user_id);
CREATE INDEX idx_audit_action ON audit_logs(action);
CREATE INDEX idx_audit_risk_level ON audit_logs(risk_level);
CREATE INDEX idx_audit_resource ON audit_logs(resource_type, resource_id);

-- Security events table
CREATE TABLE security_events (
    id INTEGER PRIMARY KEY AUTOINCREMENT,
    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    event_type TEXT NOT NULL,
    severity TEXT CHECK (severity IN ('INFO', 'WARNING', 'ERROR', 'CRITICAL')),
    source_ip TEXT,
    user_id TEXT,
    description TEXT NOT NULL,
    raw_data TEXT, -- JSON
    investigation_status TEXT DEFAULT 'OPEN',
    assigned_to TEXT,
    resolution TEXT,
    resolved_at TIMESTAMP
);
```

#### **Real-time Security Monitoring**
```javascript
// Security monitoring system
class SecurityMonitor {
  constructor() {
    this.alertThresholds = {
      failedLogins: { count: 5, window: 300000 }, // 5 attempts in 5 minutes
      fileUploads: { count: 20, window: 3600000 }, // 20 uploads in 1 hour
      apiCalls: { count: 1000, window: 3600000 }, // 1000 calls in 1 hour
      dataExports: { count: 5, window: 86400000 } // 5 exports in 24 hours
    };

    this.suspiciousPatterns = [
      /sql\s+injection/gi,
      /union\s+select/gi,
      /<script[^>]*>/gi,
      /javascript:/gi,
      /eval\s*\(/gi
    ];
  }

  // Monitor user behavior
  async monitorUserActivity(userId, action, metadata) {
    const recentActivity = await this.getRecentActivity(userId, 3600000); // 1 hour

    // Check for suspicious patterns
    const suspiciousIndicators = [
      this.checkRapidRequests(recentActivity),
      this.checkUnusualAccess(userId, metadata),
      this.checkDataExfiltration(action, metadata),
      this.checkPrivilegeEscalation(userId, action)
    ];

    const riskScore = this.calculateRiskScore(suspiciousIndicators);

    if (riskScore > 70) {
      await this.triggerSecurityAlert(userId, action, riskScore, metadata);
    }

    return riskScore;
  }

  // Automated threat detection
  async detectThreats(requestData) {
    const threats = [];

    // Check for injection attacks
    if (this.detectInjectionAttempt(requestData)) {
      threats.push({
        type: 'INJECTION_ATTEMPT',
        severity: 'HIGH',
        details: 'Potential SQL/XSS injection detected'
      });
    }

    // Check for brute force attacks
    if (await this.detectBruteForce(requestData.ip)) {
      threats.push({
        type: 'BRUTE_FORCE',
        severity: 'MEDIUM',
        details: 'Multiple failed authentication attempts'
      });
    }

    // Check for data exfiltration
    if (this.detectDataExfiltration(requestData)) {
      threats.push({
        type: 'DATA_EXFILTRATION',
        severity: 'CRITICAL',
        details: 'Unusual data access pattern detected'
      });
    }

    return threats;
  }

  // Incident response automation
  async respondToThreat(threat, context) {
    const responseActions = {
      'INJECTION_ATTEMPT': [
        () => this.blockIP(context.ip, 3600000), // 1 hour block
        () => this.alertSecurityTeam(threat),
        () => this.logSecurityEvent(threat, context)
      ],
      'BRUTE_FORCE': [
        () => this.blockIP(context.ip, 1800000), // 30 minute block
        () => this.lockUserAccount(context.userId, 900000), // 15 minute lock
        () => this.notifyUser(context.userId, 'security_alert')
      ],
      'DATA_EXFILTRATION': [
        () => this.emergencyLockdown(context.userId),
        () => this.alertManagement(threat),
        () => this.preserveEvidence(context),
        () => this.initiateInvestigation(threat)
      ]
    };

    const actions = responseActions[threat.type] || [];

    for (const action of actions) {
      try {
        await action();
      } catch (error) {
        console.error(`Failed to execute response action:`, error);
      }
    }
  }
}
```

### **Compliance Standards and Certifications**

#### **Regulatory Compliance Matrix**

| Standard | Requirement | Implementation | Status |
|----------|-------------|----------------|---------|
| **ISO 27001** | Information Security Management | Multi-layer security architecture | ‚úÖ Compliant |
| **SOC 2 Type II** | Security, Availability, Confidentiality | Audit trails, access controls | ‚úÖ Compliant |
| **GDPR** | Data Protection and Privacy | Privacy by design, data minimization | ‚úÖ Compliant |
| **PCI DSS** | Payment Card Industry Security | N/A (No card data processing) | N/A |
| **NIST Cybersecurity Framework** | Risk Management | Risk assessment tools, incident response | ‚úÖ Compliant |
| **Basel III** | Banking Regulation | Risk calculation modules | ‚úÖ Compliant |
| **MiFID II** | Financial Services Regulation | Transaction reporting, audit trails | ‚úÖ Compliant |
| **AMLD5** | Anti-Money Laundering | AML Center compliance tools | ‚úÖ Compliant |

#### **Compliance Monitoring Dashboard**
```javascript
// Compliance monitoring system
const complianceMonitor = {
  // Regulatory requirements tracking
  requirements: {
    'GDPR': {
      'data_minimization': { status: 'compliant', lastCheck: '2025-01-15' },
      'consent_management': { status: 'compliant', lastCheck: '2025-01-15' },
      'right_to_be_forgotten': { status: 'compliant', lastCheck: '2025-01-15' },
      'data_portability': { status: 'compliant', lastCheck: '2025-01-15' },
      'breach_notification': { status: 'compliant', lastCheck: '2025-01-15' }
    },
    'SOC2': {
      'access_controls': { status: 'compliant', lastCheck: '2025-01-15' },
      'system_monitoring': { status: 'compliant', lastCheck: '2025-01-15' },
      'incident_response': { status: 'compliant', lastCheck: '2025-01-15' },
      'change_management': { status: 'compliant', lastCheck: '2025-01-15' }
    },
    'ISO27001': {
      'risk_assessment': { status: 'compliant', lastCheck: '2025-01-15' },
      'security_policies': { status: 'compliant', lastCheck: '2025-01-15' },
      'employee_training': { status: 'pending', lastCheck: '2025-01-10' },
      'vendor_management': { status: 'compliant', lastCheck: '2025-01-15' }
    }
  },

  // Automated compliance checking
  async performComplianceCheck(standard) {
    const checks = this.requirements[standard];
    const results = {};

    for (const [requirement, config] of Object.entries(checks)) {
      try {
        const checkResult = await this.executeComplianceCheck(standard, requirement);
        results[requirement] = {
          status: checkResult.passed ? 'compliant' : 'non_compliant',
          score: checkResult.score,
          issues: checkResult.issues,
          recommendations: checkResult.recommendations,
          lastCheck: new Date().toISOString()
        };
      } catch (error) {
        results[requirement] = {
          status: 'error',
          error: error.message,
          lastCheck: new Date().toISOString()
        };
      }
    }

    return results;
  },

  // Generate compliance reports
  async generateComplianceReport(standard, period) {
    const report = {
      standard: standard,
      period: period,
      generatedAt: new Date().toISOString(),
      overallStatus: 'compliant',
      complianceScore: 0,
      requirements: {},
      recommendations: [],
      actionItems: []
    };

    const checks = await this.performComplianceCheck(standard);
    let totalScore = 0;
    let compliantCount = 0;

    for (const [requirement, result] of Object.entries(checks)) {
      report.requirements[requirement] = result;

      if (result.status === 'compliant') {
        compliantCount++;
        totalScore += result.score || 100;
      } else {
        report.overallStatus = 'non_compliant';
        report.actionItems.push({
          requirement: requirement,
          issue: result.issues?.[0] || 'Non-compliant status',
          priority: 'high',
          dueDate: this.calculateDueDate(requirement)
        });
      }
    }

    report.complianceScore = Math.round(totalScore / Object.keys(checks).length);

    return report;
  }
};
```

## üöÄ Deployment and Operations Guide

### **Production Deployment Architecture**

#### **High Availability Setup**

**Load Balancer Configuration**
```yaml
# docker-compose.production.yml
version: '3.8'

services:
  # Load Balancer (HAProxy)
  loadbalancer:
    image: haproxy:2.8-alpine
    ports:
      - "80:80"
      - "443:443"
      - "8404:8404"  # Stats page
    volumes:
      - ./haproxy.cfg:/usr/local/etc/haproxy/haproxy.cfg:ro
      - ./ssl:/etc/ssl/certs:ro
    depends_on:
      - crc360-app-1
      - crc360-app-2
    restart: unless-stopped
    networks:
      - crc360-network

  # Application Instance 1
  crc360-app-1:
    build:
      context: .
      dockerfile: Dockerfile.production
    environment:
      - NODE_ENV=production
      - PORT=3000
      - INSTANCE_ID=app-1
      - DATABASE_URL=postgresql://user:pass@postgres:5432/crc360
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./uploads:/app/uploads
      - ./docsecureDOCS:/app/docsecureDOCS
      - ./logs:/app/logs
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - crc360-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # Application Instance 2
  crc360-app-2:
    build:
      context: .
      dockerfile: Dockerfile.production
    environment:
      - NODE_ENV=production
      - PORT=3000
      - INSTANCE_ID=app-2
      - DATABASE_URL=postgresql://user:pass@postgres:5432/crc360
      - REDIS_URL=redis://redis:6379
    volumes:
      - ./uploads:/app/uploads
      - ./docsecureDOCS:/app/docsecureDOCS
      - ./logs:/app/logs
    depends_on:
      - postgres
      - redis
    restart: unless-stopped
    networks:
      - crc360-network
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # PostgreSQL Database
  postgres:
    image: postgres:15-alpine
    environment:
      - POSTGRES_DB=crc360
      - POSTGRES_USER=crc360_user
      - POSTGRES_PASSWORD=${DB_PASSWORD}
      - POSTGRES_INITDB_ARGS=--encoding=UTF-8 --lc-collate=C --lc-ctype=C
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./init-scripts:/docker-entrypoint-initdb.d
    restart: unless-stopped
    networks:
      - crc360-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U crc360_user -d crc360"]
      interval: 10s
      timeout: 5s
      retries: 5

  # Redis Cache
  redis:
    image: redis:7-alpine
    command: redis-server --appendonly yes --requirepass ${REDIS_PASSWORD}
    volumes:
      - redis_data:/data
    restart: unless-stopped
    networks:
      - crc360-network
    healthcheck:
      test: ["CMD", "redis-cli", "--raw", "incr", "ping"]
      interval: 10s
      timeout: 3s
      retries: 5

  # Monitoring (Prometheus)
  prometheus:
    image: prom/prometheus:latest
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--storage.tsdb.retention.time=200h'
      - '--web.enable-lifecycle'
    restart: unless-stopped
    networks:
      - crc360-network

  # Grafana Dashboard
  grafana:
    image: grafana/grafana:latest
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_PASSWORD}
      - GF_USERS_ALLOW_SIGN_UP=false
    volumes:
      - grafana_data:/var/lib/grafana
      - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
      - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
    restart: unless-stopped
    networks:
      - crc360-network

volumes:
  postgres_data:
  redis_data:
  prometheus_data:
  grafana_data:

networks:
  crc360-network:
    driver: bridge
```

**HAProxy Configuration**
```
# haproxy.cfg
global
    daemon
    log stdout local0
    chroot /var/lib/haproxy
    stats socket /run/haproxy/admin.sock mode 660 level admin
    stats timeout 30s
    user haproxy
    group haproxy

    # SSL Configuration
    ssl-default-bind-ciphers ECDHE+AESGCM:ECDHE+CHACHA20:RSA+AESGCM:RSA+AES:!aNULL:!MD5:!DSS
    ssl-default-bind-options ssl-min-ver TLSv1.2 no-tls-tickets

defaults
    mode http
    log global
    option httplog
    option dontlognull
    option log-health-checks
    option forwardfor
    option http-server-close
    timeout connect 5000
    timeout client 50000
    timeout server 50000
    errorfile 400 /etc/haproxy/errors/400.http
    errorfile 403 /etc/haproxy/errors/403.http
    errorfile 408 /etc/haproxy/errors/408.http
    errorfile 500 /etc/haproxy/errors/500.http
    errorfile 502 /etc/haproxy/errors/502.http
    errorfile 503 /etc/haproxy/errors/503.http
    errorfile 504 /etc/haproxy/errors/504.http

# Statistics page
frontend stats
    bind *:8404
    stats enable
    stats uri /stats
    stats refresh 30s
    stats admin if TRUE

# HTTP frontend (redirect to HTTPS)
frontend http_frontend
    bind *:80
    redirect scheme https code 301 if !{ ssl_fc }

# HTTPS frontend
frontend https_frontend
    bind *:443 ssl crt /etc/ssl/certs/crc360.pem

    # Security headers
    http-response set-header Strict-Transport-Security "max-age=31536000; includeSubDomains; preload"
    http-response set-header X-Frame-Options "DENY"
    http-response set-header X-Content-Type-Options "nosniff"
    http-response set-header X-XSS-Protection "1; mode=block"
    http-response set-header Referrer-Policy "strict-origin-when-cross-origin"

    # Rate limiting
    stick-table type ip size 100k expire 30s store http_req_rate(10s)
    http-request track-sc0 src
    http-request reject if { sc_http_req_rate(0) gt 20 }

    # Health check endpoint
    acl health_check path_beg /api/health
    use_backend health_backend if health_check

    # API endpoints
    acl api_request path_beg /api/
    use_backend api_backend if api_request

    # Default backend
    default_backend web_backend

# Backend for health checks
backend health_backend
    balance roundrobin
    option httpchk GET /api/health
    http-check expect status 200
    server app1 crc360-app-1:3000 check inter 10s
    server app2 crc360-app-2:3000 check inter 10s

# Backend for API requests
backend api_backend
    balance roundrobin
    option httpchk GET /api/health
    http-check expect status 200
    timeout server 60s
    server app1 crc360-app-1:3000 check inter 10s
    server app2 crc360-app-2:3000 check inter 10s

# Backend for web requests
backend web_backend
    balance roundrobin
    option httpchk GET /api/health
    http-check expect status 200
    server app1 crc360-app-1:3000 check inter 10s
    server app2 crc360-app-2:3000 check inter 10s
```

#### **Container Optimization**

**Production Dockerfile**
```dockerfile
# Dockerfile.production
# Multi-stage build for optimized production image

# Stage 1: Dependencies
FROM node:20-alpine AS deps
RUN apk add --no-cache libc6-compat python3 make g++
WORKDIR /app

# Copy package files
COPY package.json pnpm-lock.yaml* ./
RUN corepack enable pnpm && pnpm install --frozen-lockfile --production=false

# Stage 2: Builder
FROM node:20-alpine AS builder
WORKDIR /app
COPY --from=deps /app/node_modules ./node_modules
COPY . .

# Build the application
ENV NEXT_TELEMETRY_DISABLED 1
ENV NODE_ENV production
RUN corepack enable pnpm && pnpm build

# Stage 3: Runner
FROM node:20-alpine AS runner
WORKDIR /app

# Create non-root user
RUN addgroup --system --gid 1001 nodejs
RUN adduser --system --uid 1001 nextjs

# Install runtime dependencies
RUN apk add --no-cache python3 py3-pip curl
RUN pip3 install --no-cache-dir sqlite3

# Copy built application
COPY --from=builder /app/public ./public
COPY --from=builder /app/.next/standalone ./
COPY --from=builder /app/.next/static ./.next/static
COPY --from=builder /app/integrated-server.js ./
COPY --from=builder /app/app/amlcenter ./app/amlcenter
COPY --from=builder /app/utils ./utils
COPY --from=builder /app/scripts ./scripts

# Create necessary directories
RUN mkdir -p uploads docsecureDOCS logs
RUN chown -R nextjs:nodejs /app

# Switch to non-root user
USER nextjs

# Expose port
EXPOSE 3000

# Health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
  CMD curl -f http://localhost:3000/api/health || exit 1

# Start the application
CMD ["node", "integrated-server.js"]
```

### **Monitoring and Observability**

#### **Application Performance Monitoring**

**Prometheus Metrics Configuration**
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s
  evaluation_interval: 15s

rule_files:
  - "rules/*.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093

scrape_configs:
  # CRC360 Application Metrics
  - job_name: 'crc360-app'
    static_configs:
      - targets: ['crc360-app-1:3000', 'crc360-app-2:3000']
    metrics_path: '/api/metrics'
    scrape_interval: 10s

  # Node Exporter (System Metrics)
  - job_name: 'node-exporter'
    static_configs:
      - targets: ['node-exporter:9100']

  # PostgreSQL Metrics
  - job_name: 'postgres-exporter'
    static_configs:
      - targets: ['postgres-exporter:9187']

  # Redis Metrics
  - job_name: 'redis-exporter'
    static_configs:
      - targets: ['redis-exporter:9121']

  # HAProxy Metrics
  - job_name: 'haproxy'
    static_configs:
      - targets: ['loadbalancer:8404']
    metrics_path: '/stats/prometheus'
```

**Custom Application Metrics**
```javascript
// lib/metrics.js
const promClient = require('prom-client');

// Create a Registry
const register = new promClient.Registry();

// Add default metrics
promClient.collectDefaultMetrics({
  register,
  prefix: 'crc360_',
});

// Custom metrics
const httpRequestDuration = new promClient.Histogram({
  name: 'crc360_http_request_duration_seconds',
  help: 'Duration of HTTP requests in seconds',
  labelNames: ['method', 'route', 'status_code'],
  buckets: [0.1, 0.5, 1, 2, 5, 10]
});

const httpRequestTotal = new promClient.Counter({
  name: 'crc360_http_requests_total',
  help: 'Total number of HTTP requests',
  labelNames: ['method', 'route', 'status_code']
});

const activeUsers = new promClient.Gauge({
  name: 'crc360_active_users',
  help: 'Number of active users'
});

const fileUploads = new promClient.Counter({
  name: 'crc360_file_uploads_total',
  help: 'Total number of file uploads',
  labelNames: ['module', 'file_type', 'status']
});

const documentCount = new promClient.Gauge({
  name: 'crc360_documents_total',
  help: 'Total number of documents in the system',
  labelNames: ['category']
});

const amlProcessingTime = new promClient.Histogram({
  name: 'crc360_aml_processing_duration_seconds',
  help: 'Time taken to process AML files',
  labelNames: ['file_type'],
  buckets: [1, 5, 10, 30, 60, 120, 300]
});

const systemHealth = new promClient.Gauge({
  name: 'crc360_system_health',
  help: 'System health status (1 = healthy, 0 = unhealthy)',
  labelNames: ['component']
});

// Register metrics
register.registerMetric(httpRequestDuration);
register.registerMetric(httpRequestTotal);
register.registerMetric(activeUsers);
register.registerMetric(fileUploads);
register.registerMetric(documentCount);
register.registerMetric(amlProcessingTime);
register.registerMetric(systemHealth);

// Middleware for HTTP metrics
const metricsMiddleware = (req, res, next) => {
  const start = Date.now();

  res.on('finish', () => {
    const duration = (Date.now() - start) / 1000;
    const route = req.route?.path || req.path;

    httpRequestDuration
      .labels(req.method, route, res.statusCode)
      .observe(duration);

    httpRequestTotal
      .labels(req.method, route, res.statusCode)
      .inc();
  });

  next();
};

// Health check function
const updateSystemHealth = async () => {
  try {
    // Check database connectivity
    const dbHealth = await checkDatabaseHealth();
    systemHealth.labels('database').set(dbHealth ? 1 : 0);

    // Check file system
    const fsHealth = await checkFileSystemHealth();
    systemHealth.labels('filesystem').set(fsHealth ? 1 : 0);

    // Check external services
    const extHealth = await checkExternalServices();
    systemHealth.labels('external_services').set(extHealth ? 1 : 0);

  } catch (error) {
    console.error('Health check failed:', error);
    systemHealth.labels('overall').set(0);
  }
};

// Update health metrics every 30 seconds
setInterval(updateSystemHealth, 30000);

module.exports = {
  register,
  metrics: {
    httpRequestDuration,
    httpRequestTotal,
    activeUsers,
    fileUploads,
    documentCount,
    amlProcessingTime,
    systemHealth
  },
  metricsMiddleware,
  updateSystemHealth
};
```

#### **Logging and Error Tracking**

**Structured Logging Configuration**
```javascript
// lib/logger.js
const winston = require('winston');
const path = require('path');

// Custom log format
const logFormat = winston.format.combine(
  winston.format.timestamp({
    format: 'YYYY-MM-DD HH:mm:ss.SSS'
  }),
  winston.format.errors({ stack: true }),
  winston.format.json(),
  winston.format.printf(({ timestamp, level, message, ...meta }) => {
    return JSON.stringify({
      timestamp,
      level,
      message,
      ...meta,
      service: 'crc360',
      version: process.env.APP_VERSION || '1.0.0',
      instance: process.env.INSTANCE_ID || 'unknown'
    });
  })
);

// Create logger
const logger = winston.createLogger({
  level: process.env.LOG_LEVEL || 'info',
  format: logFormat,
  defaultMeta: {
    service: 'crc360'
  },
  transports: [
    // Console output
    new winston.transports.Console({
      format: winston.format.combine(
        winston.format.colorize(),
        winston.format.simple()
      )
    }),

    // File outputs
    new winston.transports.File({
      filename: path.join('logs', 'error.log'),
      level: 'error',
      maxsize: 10485760, // 10MB
      maxFiles: 5
    }),
    new winston.transports.File({
      filename: path.join('logs', 'combined.log'),
      maxsize: 10485760, // 10MB
      maxFiles: 10
    }),

    // Security events
    new winston.transports.File({
      filename: path.join('logs', 'security.log'),
      level: 'warn',
      maxsize: 10485760, // 10MB
      maxFiles: 20
    })
  ],

  // Handle uncaught exceptions
  exceptionHandlers: [
    new winston.transports.File({
      filename: path.join('logs', 'exceptions.log')
    })
  ],

  // Handle unhandled promise rejections
  rejectionHandlers: [
    new winston.transports.File({
      filename: path.join('logs', 'rejections.log')
    })
  ]
});

// Request logging middleware
const requestLogger = (req, res, next) => {
  const start = Date.now();

  // Log request
  logger.info('HTTP Request', {
    method: req.method,
    url: req.url,
    userAgent: req.get('User-Agent'),
    ip: req.ip,
    userId: req.user?.id,
    sessionId: req.sessionID
  });

  // Log response
  res.on('finish', () => {
    const duration = Date.now() - start;

    logger.info('HTTP Response', {
      method: req.method,
      url: req.url,
      statusCode: res.statusCode,
      duration: `${duration}ms`,
      userId: req.user?.id,
      sessionId: req.sessionID
    });
  });

  next();
};

// Security event logger
const logSecurityEvent = (event, details) => {
  logger.warn('Security Event', {
    event,
    ...details,
    timestamp: new Date().toISOString()
  });
};

// Audit logger
const logAuditEvent = (action, resource, user, details) => {
  logger.info('Audit Event', {
    action,
    resource,
    user: {
      id: user.id,
      username: user.username,
      role: user.role
    },
    details,
    timestamp: new Date().toISOString()
  });
};

module.exports = {
  logger,
  requestLogger,
  logSecurityEvent,
  logAuditEvent
};
```

### **Backup and Disaster Recovery**

#### **Automated Backup System**
```bash
#!/bin/bash
# scripts/backup.sh

set -euo pipefail

# Configuration
BACKUP_DIR="/backups"
RETENTION_DAYS=30
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="crc360_backup_${DATE}"

# Create backup directory
mkdir -p "${BACKUP_DIR}/${BACKUP_NAME}"

echo "Starting CRC360 backup at $(date)"

# 1. Database backup
echo "Backing up PostgreSQL database..."
pg_dump -h postgres -U crc360_user -d crc360 | gzip > "${BACKUP_DIR}/${BACKUP_NAME}/database.sql.gz"

# 2. SQLite databases backup
echo "Backing up SQLite databases..."
cp docsecureDOCS/metadata.db "${BACKUP_DIR}/${BACKUP_NAME}/docsecure_metadata.db"

# 3. Document files backup
echo "Backing up document files..."
tar -czf "${BACKUP_DIR}/${BACKUP_NAME}/documents.tar.gz" docsecureDOCS/

# 4. Upload files backup
echo "Backing up upload files..."
tar -czf "${BACKUP_DIR}/${BACKUP_NAME}/uploads.tar.gz" uploads/

# 5. Configuration backup
echo "Backing up configuration files..."
tar -czf "${BACKUP_DIR}/${BACKUP_NAME}/config.tar.gz" \
  .env.production \
  docker-compose.production.yml \
  haproxy.cfg \
  monitoring/

# 6. Application logs backup
echo "Backing up logs..."
tar -czf "${BACKUP_DIR}/${BACKUP_NAME}/logs.tar.gz" logs/

# 7. Create backup manifest
echo "Creating backup manifest..."
cat > "${BACKUP_DIR}/${BACKUP_NAME}/manifest.json" << EOF
{
  "backup_name": "${BACKUP_NAME}",
  "timestamp": "$(date -Iseconds)",
  "version": "$(git rev-parse HEAD 2>/dev/null || echo 'unknown')",
  "components": [
    "database",
    "docsecure_metadata",
    "documents",
    "uploads",
    "config",
    "logs"
  ],
  "size": "$(du -sh ${BACKUP_DIR}/${BACKUP_NAME} | cut -f1)"
}
EOF

# 8. Encrypt backup
echo "Encrypting backup..."
tar -czf - -C "${BACKUP_DIR}" "${BACKUP_NAME}" | \
  gpg --symmetric --cipher-algo AES256 --compress-algo 1 --s2k-mode 3 \
      --s2k-digest-algo SHA512 --s2k-count 65536 --force-mdc \
      --output "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz.gpg"

# 9. Remove unencrypted backup
rm -rf "${BACKUP_DIR}/${BACKUP_NAME}"

# 10. Upload to cloud storage (optional)
if [ "${CLOUD_BACKUP_ENABLED:-false}" = "true" ]; then
  echo "Uploading to cloud storage..."
  aws s3 cp "${BACKUP_DIR}/${BACKUP_NAME}.tar.gz.gpg" \
    "s3://${BACKUP_BUCKET}/crc360/${BACKUP_NAME}.tar.gz.gpg"
fi

# 11. Cleanup old backups
echo "Cleaning up old backups..."
find "${BACKUP_DIR}" -name "crc360_backup_*.tar.gz.gpg" -mtime +${RETENTION_DAYS} -delete

echo "Backup completed successfully at $(date)"
echo "Backup location: ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz.gpg"
```

#### **Disaster Recovery Procedures**
```bash
#!/bin/bash
# scripts/restore.sh

set -euo pipefail

BACKUP_FILE="$1"
RESTORE_DIR="/tmp/crc360_restore_$(date +%s)"

if [ -z "${BACKUP_FILE}" ]; then
  echo "Usage: $0 <backup_file.tar.gz.gpg>"
  exit 1
fi

echo "Starting CRC360 restore from ${BACKUP_FILE}"

# 1. Decrypt and extract backup
echo "Decrypting and extracting backup..."
mkdir -p "${RESTORE_DIR}"
gpg --decrypt "${BACKUP_FILE}" | tar -xzf - -C "${RESTORE_DIR}"

BACKUP_NAME=$(ls "${RESTORE_DIR}")
BACKUP_PATH="${RESTORE_DIR}/${BACKUP_NAME}"

# 2. Verify backup integrity
echo "Verifying backup integrity..."
if [ ! -f "${BACKUP_PATH}/manifest.json" ]; then
  echo "Error: Invalid backup file - missing manifest"
  exit 1
fi

# 3. Stop services
echo "Stopping CRC360 services..."
docker-compose -f docker-compose.production.yml down

# 4. Restore database
echo "Restoring PostgreSQL database..."
gunzip -c "${BACKUP_PATH}/database.sql.gz" | \
  docker-compose -f docker-compose.production.yml exec -T postgres \
  psql -U crc360_user -d crc360

# 5. Restore SQLite databases
echo "Restoring SQLite databases..."
cp "${BACKUP_PATH}/docsecure_metadata.db" docsecureDOCS/metadata.db

# 6. Restore document files
echo "Restoring document files..."
tar -xzf "${BACKUP_PATH}/documents.tar.gz"

# 7. Restore upload files
echo "Restoring upload files..."
tar -xzf "${BACKUP_PATH}/uploads.tar.gz"

# 8. Restore configuration
echo "Restoring configuration..."
tar -xzf "${BACKUP_PATH}/config.tar.gz"

# 9. Set proper permissions
echo "Setting file permissions..."
chown -R 1001:1001 docsecureDOCS/ uploads/
chmod -R 750 docsecureDOCS/
chmod -R 755 uploads/

# 10. Start services
echo "Starting CRC360 services..."
docker-compose -f docker-compose.production.yml up -d

# 11. Verify restoration
echo "Verifying restoration..."
sleep 30
if curl -f http://localhost/api/health; then
  echo "Restoration completed successfully!"
else
  echo "Warning: Health check failed after restoration"
fi

# 12. Cleanup
rm -rf "${RESTORE_DIR}"

echo "Restore process completed at $(date)"
```

## üîç Troubleshooting and Maintenance Guide

### **Common Issues and Solutions**

#### **Application Startup Issues**

**Issue: Port Already in Use**
```bash
# Problem: Error: listen EADDRINUSE: address already in use :::3000
# Solution:
# 1. Find process using port 3000
lsof -ti:3000

# 2. Kill the process
kill -9 $(lsof -ti:3000)

# 3. Or use a different port
PORT=3001 npm run dev
```

**Issue: Database Connection Failed**
```bash
# Problem: Database connection errors
# Solution:
# 1. Check database status
docker-compose ps postgres

# 2. Check database logs
docker-compose logs postgres

# 3. Restart database service
docker-compose restart postgres

# 4. Verify connection string
echo $DATABASE_URL
```

**Issue: File Upload Failures**
```bash
# Problem: File uploads failing or timing out
# Solution:
# 1. Check upload directory permissions
ls -la uploads/
chmod 755 uploads/

# 2. Check disk space
df -h

# 3. Check file size limits
grep -r "50.*MB" .

# 4. Check server logs
tail -f logs/error.log
```

#### **Performance Issues**

**Issue: Slow Page Load Times**
```javascript
// Diagnostic steps:
// 1. Check server response times
const performanceCheck = async () => {
  const start = performance.now();
  const response = await fetch('/api/health');
  const end = performance.now();
  console.log(`API response time: ${end - start}ms`);
};

// 2. Monitor memory usage
const checkMemoryUsage = () => {
  const used = process.memoryUsage();
  console.log('Memory Usage:');
  for (let key in used) {
    console.log(`${key}: ${Math.round(used[key] / 1024 / 1024 * 100) / 100} MB`);
  }
};

// 3. Check database query performance
// Enable query logging in PostgreSQL
// Add to postgresql.conf:
// log_statement = 'all'
// log_min_duration_statement = 1000  # Log queries taking > 1s
```

**Issue: High Memory Usage**
```bash
# Problem: Application consuming too much memory
# Solution:
# 1. Monitor memory usage
docker stats

# 2. Check for memory leaks
node --inspect integrated-server.js
# Open Chrome DevTools -> Memory tab

# 3. Increase memory limit
node --max-old-space-size=4096 integrated-server.js

# 4. Restart application
pm2 restart crc360
```

#### **Module-Specific Issues**

**Rep Watch Issues**
```html
<!-- Problem: Calendar heatmap not displaying -->
<!-- Solution: Check Plotly.js loading -->
<script>
// Debug Plotly loading
if (typeof Plotly === 'undefined') {
  console.error('Plotly.js not loaded');
  // Reload Plotly
  const script = document.createElement('script');
  script.src = 'https://cdn.plot.ly/plotly-latest.min.js';
  document.head.appendChild(script);
}

// Check data format
console.log('Calendar data:', calendarData);
</script>
```

**AML Center Issues**
```javascript
// Problem: PDF processing failures
// Solution: Debug PDF parsing
const debugPdfProcessing = async (file) => {
  try {
    console.log('File info:', {
      name: file.name,
      size: file.size,
      type: file.type
    });

    const formData = new FormData();
    formData.append('pdf', file);

    const response = await fetch('/api/aml/upload-pdf', {
      method: 'POST',
      body: formData
    });

    if (!response.ok) {
      const error = await response.text();
      console.error('Upload failed:', error);
    }
  } catch (error) {
    console.error('Processing error:', error);
  }
};
```

**Doc Secure Issues**
```python
# Problem: Database corruption
# Solution: Repair SQLite database
import sqlite3
import shutil

def repair_database():
    db_path = "docsecureDOCS/metadata.db"
    backup_path = f"{db_path}.backup"

    # Create backup
    shutil.copy2(db_path, backup_path)

    try:
        # Check integrity
        conn = sqlite3.connect(db_path)
        cursor = conn.cursor()
        cursor.execute("PRAGMA integrity_check")
        result = cursor.fetchone()

        if result[0] != "ok":
            print("Database corruption detected")
            # Attempt repair
            cursor.execute("REINDEX")
            cursor.execute("VACUUM")
            conn.commit()

        conn.close()
        print("Database repair completed")

    except Exception as e:
        print(f"Repair failed: {e}")
        # Restore backup
        shutil.copy2(backup_path, db_path)
```

**R-Sense Issues**
```javascript
// Problem: Chart rendering failures
// Solution: Debug chart components
const debugChartRendering = () => {
  // Check data availability
  console.log('Chart data:', {
    riskData: window.riskData,
    assetData: window.assetData,
    controlData: window.controlData
  });

  // Check container elements
  const containers = [
    'risk-matrix-container',
    'asset-chart-container',
    'control-chart-container'
  ];

  containers.forEach(id => {
    const element = document.getElementById(id);
    if (!element) {
      console.error(`Container not found: ${id}`);
    } else {
      console.log(`Container ${id}:`, element.getBoundingClientRect());
    }
  });

  // Force re-render
  if (typeof renderCharts === 'function') {
    renderCharts();
  }
};
```

### **Maintenance Procedures**

#### **Regular Maintenance Tasks**

**Daily Tasks**
```bash
#!/bin/bash
# scripts/daily-maintenance.sh

echo "Starting daily maintenance at $(date)"

# 1. Check system health
curl -f http://localhost:3000/api/health || echo "Health check failed"

# 2. Check disk space
df -h | awk '$5 > 80 {print "Warning: " $0}'

# 3. Check log file sizes
find logs/ -name "*.log" -size +100M -exec echo "Large log file: {}" \;

# 4. Rotate logs if needed
if [ -f logs/combined.log ] && [ $(stat -f%z logs/combined.log 2>/dev/null || stat -c%s logs/combined.log) -gt 104857600 ]; then
  mv logs/combined.log logs/combined.log.$(date +%Y%m%d)
  touch logs/combined.log
fi

# 5. Check for failed uploads
find uploads/ -name "*.tmp" -mtime +1 -delete

# 6. Update document count metrics
python3 scripts/update-metrics.py

echo "Daily maintenance completed at $(date)"
```

**Weekly Tasks**
```bash
#!/bin/bash
# scripts/weekly-maintenance.sh

echo "Starting weekly maintenance at $(date)"

# 1. Database optimization
echo "Optimizing databases..."
sqlite3 docsecureDOCS/metadata.db "VACUUM; REINDEX;"

# 2. Clean up old logs
find logs/ -name "*.log.*" -mtime +7 -delete

# 3. Check for security updates
npm audit --audit-level moderate

# 4. Backup verification
if [ -f "/backups/$(ls /backups | tail -1)" ]; then
  echo "Latest backup: $(ls -lt /backups | head -2 | tail -1)"
else
  echo "Warning: No recent backups found"
fi

# 5. Performance analysis
echo "Generating performance report..."
node scripts/performance-report.js

echo "Weekly maintenance completed at $(date)"
```

**Monthly Tasks**
```bash
#!/bin/bash
# scripts/monthly-maintenance.sh

echo "Starting monthly maintenance at $(date)"

# 1. Full system backup
./scripts/backup.sh

# 2. Security scan
npm audit --audit-level low

# 3. Dependency updates (review only)
npm outdated

# 4. Database statistics
echo "Database statistics:"
sqlite3 docsecureDOCS/metadata.db "
  SELECT
    'Total documents' as metric,
    COUNT(*) as value
  FROM documents
  UNION ALL
  SELECT
    'Storage used (MB)',
    ROUND(SUM(file_size)/1024.0/1024.0, 2)
  FROM documents;
"

# 5. Generate monthly report
node scripts/monthly-report.js

echo "Monthly maintenance completed at $(date)"
```

#### **Database Maintenance**

**PostgreSQL Maintenance**
```sql
-- Monthly PostgreSQL maintenance
-- Run these queries during low-traffic periods

-- 1. Update table statistics
ANALYZE;

-- 2. Reindex tables
REINDEX DATABASE crc360;

-- 3. Check for bloat
SELECT
  schemaname,
  tablename,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) as size,
  pg_size_pretty(pg_relation_size(schemaname||'.'||tablename)) as table_size,
  pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename) - pg_relation_size(schemaname||'.'||tablename)) as index_size
FROM pg_tables
WHERE schemaname = 'public'
ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC;

-- 4. Check slow queries
SELECT
  query,
  calls,
  total_time,
  mean_time,
  rows
FROM pg_stat_statements
ORDER BY mean_time DESC
LIMIT 10;

-- 5. Vacuum analyze
VACUUM ANALYZE;
```

**SQLite Maintenance**
```python
# scripts/sqlite-maintenance.py
import sqlite3
import os
from datetime import datetime, timedelta

def maintain_sqlite_db(db_path):
    """Perform SQLite database maintenance"""

    print(f"Maintaining database: {db_path}")

    # Backup before maintenance
    backup_path = f"{db_path}.backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
    os.system(f"cp {db_path} {backup_path}")

    conn = sqlite3.connect(db_path)
    cursor = conn.cursor()

    try:
        # 1. Integrity check
        print("Checking database integrity...")
        cursor.execute("PRAGMA integrity_check")
        integrity_result = cursor.fetchone()
        if integrity_result[0] != "ok":
            print(f"Integrity check failed: {integrity_result[0]}")
            return False

        # 2. Analyze database
        print("Analyzing database...")
        cursor.execute("ANALYZE")

        # 3. Vacuum database
        print("Vacuuming database...")
        cursor.execute("VACUUM")

        # 4. Update statistics
        cursor.execute("PRAGMA optimize")

        # 5. Check database size
        cursor.execute("PRAGMA page_count")
        page_count = cursor.fetchone()[0]
        cursor.execute("PRAGMA page_size")
        page_size = cursor.fetchone()[0]
        db_size = page_count * page_size

        print(f"Database size: {db_size / 1024 / 1024:.2f} MB")

        # 6. Clean up old audit records (keep last 6 months)
        six_months_ago = datetime.now() - timedelta(days=180)
        cursor.execute(
            "DELETE FROM document_audit WHERE timestamp < ?",
            (six_months_ago.isoformat(),)
        )
        deleted_records = cursor.rowcount
        print(f"Cleaned up {deleted_records} old audit records")

        conn.commit()
        print("Database maintenance completed successfully")

        # Remove backup if successful
        os.remove(backup_path)

        return True

    except Exception as e:
        print(f"Database maintenance failed: {e}")
        # Restore backup
        os.system(f"cp {backup_path} {db_path}")
        return False

    finally:
        conn.close()

if __name__ == "__main__":
    maintain_sqlite_db("docsecureDOCS/metadata.db")
```

### **Performance Optimization**

#### **Frontend Optimization**
```javascript
// Performance optimization techniques

// 1. Lazy loading for large components
const LazyRepWatch = lazy(() => import('./components/rep-watch/Dashboard'));
const LazyAMLCenter = lazy(() => import('./components/aml/Dashboard'));

// 2. Memoization for expensive calculations
const MemoizedChart = memo(({ data }) => {
  const processedData = useMemo(() => {
    return data.map(item => ({
      ...item,
      calculated: expensiveCalculation(item)
    }));
  }, [data]);

  return <Chart data={processedData} />;
});

// 3. Virtual scrolling for large lists
const VirtualizedDocumentList = ({ documents }) => {
  const rowRenderer = ({ index, key, style }) => (
    <div key={key} style={style}>
      <DocumentItem document={documents[index]} />
    </div>
  );

  return (
    <AutoSizer>
      {({ height, width }) => (
        <List
          height={height}
          width={width}
          rowCount={documents.length}
          rowHeight={60}
          rowRenderer={rowRenderer}
        />
      )}
    </AutoSizer>
  );
};

// 4. Debounced search
const useDebounceSearch = (searchTerm, delay = 300) => {
  const [debouncedTerm, setDebouncedTerm] = useState(searchTerm);

  useEffect(() => {
    const timer = setTimeout(() => {
      setDebouncedTerm(searchTerm);
    }, delay);

    return () => clearTimeout(timer);
  }, [searchTerm, delay]);

  return debouncedTerm;
};
```

#### **Backend Optimization**
```javascript
// Backend performance optimizations

// 1. Database connection pooling
const pool = new Pool({
  host: process.env.DB_HOST,
  port: process.env.DB_PORT,
  database: process.env.DB_NAME,
  user: process.env.DB_USER,
  password: process.env.DB_PASSWORD,
  max: 20, // Maximum number of connections
  idleTimeoutMillis: 30000,
  connectionTimeoutMillis: 2000,
});

// 2. Redis caching
const redis = require('redis');
const client = redis.createClient({
  host: process.env.REDIS_HOST,
  port: process.env.REDIS_PORT,
  password: process.env.REDIS_PASSWORD
});

const cacheMiddleware = (duration = 300) => {
  return async (req, res, next) => {
    const key = `cache:${req.originalUrl}`;

    try {
      const cached = await client.get(key);
      if (cached) {
        return res.json(JSON.parse(cached));
      }

      // Store original json method
      const originalJson = res.json;

      // Override json method to cache response
      res.json = function(data) {
        client.setex(key, duration, JSON.stringify(data));
        return originalJson.call(this, data);
      };

      next();
    } catch (error) {
      console.error('Cache error:', error);
      next();
    }
  };
};

// 3. File processing optimization
const processFileAsync = async (filePath) => {
  return new Promise((resolve, reject) => {
    const worker = new Worker('./workers/file-processor.js');

    worker.postMessage({ filePath });

    worker.on('message', (result) => {
      worker.terminate();
      resolve(result);
    });

    worker.on('error', (error) => {
      worker.terminate();
      reject(error);
    });

    // Timeout after 5 minutes
    setTimeout(() => {
      worker.terminate();
      reject(new Error('File processing timeout'));
    }, 300000);
  });
};

// 4. Streaming for large responses
const streamLargeData = (req, res) => {
  res.writeHead(200, {
    'Content-Type': 'application/json',
    'Transfer-Encoding': 'chunked'
  });

  res.write('[');

  let first = true;
  const stream = database.createReadStream();

  stream.on('data', (chunk) => {
    if (!first) res.write(',');
    res.write(JSON.stringify(chunk));
    first = false;
  });

  stream.on('end', () => {
    res.write(']');
    res.end();
  });

  stream.on('error', (error) => {
    res.status(500).json({ error: error.message });
  });
};
```

## üìö API Documentation and Integration Guide

### **Core API Endpoints**

#### **System Health and Status**

**GET /api/health**
```javascript
// Health check endpoint
// Returns: System health status and component availability

// Response format:
{
  "status": "healthy",
  "timestamp": "2025-01-15T10:30:00.000Z",
  "version": "1.0.0",
  "uptime": 86400,
  "components": {
    "database": {
      "status": "healthy",
      "responseTime": 15,
      "lastCheck": "2025-01-15T10:29:55.000Z"
    },
    "filesystem": {
      "status": "healthy",
      "freeSpace": "15.2GB",
      "totalSpace": "50GB"
    },
    "memory": {
      "status": "healthy",
      "used": "512MB",
      "total": "2GB",
      "percentage": 25.6
    }
  }
}

// Usage example:
const checkHealth = async () => {
  try {
    const response = await fetch('/api/health');
    const health = await response.json();

    if (health.status === 'healthy') {
      console.log('System is healthy');
    } else {
      console.warn('System health issues detected:', health.components);
    }
  } catch (error) {
    console.error('Health check failed:', error);
  }
};
```

#### **Authentication API**

**POST /api/admin/auth/login**
```javascript
// Admin authentication
// Body: { username: string, password: string }
// Returns: Authentication token and user information

const login = async (username, password) => {
  const response = await fetch('/api/admin/auth/login', {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify({ username, password }),
  });

  if (response.ok) {
    const result = await response.json();
    // Store token for future requests
    localStorage.setItem('auth-token', result.token);
    return result;
  } else {
    throw new Error('Authentication failed');
  }
};
```

**GET /api/admin/auth/verify**
```javascript
// Verify authentication status
// Headers: Authorization: Bearer <token>
// Returns: User information if authenticated

const verifyAuth = async () => {
  const token = localStorage.getItem('auth-token');

  const response = await fetch('/api/admin/auth/verify', {
    headers: {
      'Authorization': `Bearer ${token}`,
    },
  });

  return response.ok;
};
```

#### **Document Management API**

**POST /api/docsecure/upload**
```javascript
// Upload document to Doc Secure
// Content-Type: multipart/form-data
// Fields: file, title, category, description

const uploadDocument = async (file, metadata) => {
  const formData = new FormData();
  formData.append('file', file);
  formData.append('title', metadata.title);
  formData.append('category', metadata.category);
  formData.append('description', metadata.description || '');

  const response = await fetch('/api/docsecure/upload', {
    method: 'POST',
    body: formData,
  });

  if (response.ok) {
    const result = await response.json();
    return result;
  } else {
    const error = await response.json();
    throw new Error(error.message);
  }
};

// Response format:
{
  "success": true,
  "document": {
    "id": 123,
    "title": "Risk Management Procedure",
    "category": "Proc√©dures",
    "filename": "secure_filename_123.pdf",
    "size": 2048576,
    "hash": "sha256_hash_here",
    "uploadDate": "2025-01-15T10:30:00.000Z"
  }
}
```

**GET /api/docsecure/documents**
```javascript
// List documents with optional filtering
// Query parameters: category, search, page, limit, sortBy, sortOrder

const getDocuments = async (filters = {}) => {
  const params = new URLSearchParams();

  Object.entries(filters).forEach(([key, value]) => {
    if (value) params.append(key, value);
  });

  const response = await fetch(`/api/docsecure/documents?${params}`);

  if (response.ok) {
    return await response.json();
  } else {
    throw new Error('Failed to fetch documents');
  }
};

// Response format:
{
  "documents": [
    {
      "id": 123,
      "title": "Risk Management Procedure",
      "category": "Proc√©dures",
      "description": "Comprehensive risk management procedures",
      "filename": "risk_management.pdf",
      "size": 2048576,
      "uploadDate": "2025-01-15T10:30:00.000Z",
      "lastModified": "2025-01-15T10:30:00.000Z"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 20,
    "total": 150,
    "pages": 8
  }
}
```

**GET /api/docsecure/download/[id]**
```javascript
// Download document by ID
// Returns: File stream with appropriate headers

const downloadDocument = async (documentId) => {
  const response = await fetch(`/api/docsecure/download/${documentId}`);

  if (response.ok) {
    const blob = await response.blob();
    const filename = response.headers.get('Content-Disposition')
      ?.split('filename=')[1]?.replace(/"/g, '') || 'document';

    // Create download link
    const url = window.URL.createObjectURL(blob);
    const a = document.createElement('a');
    a.href = url;
    a.download = filename;
    document.body.appendChild(a);
    a.click();
    window.URL.revokeObjectURL(url);
    document.body.removeChild(a);
  } else {
    throw new Error('Download failed');
  }
};
```

#### **AML Center API**

**POST /api/aml/upload-pdf**
```javascript
// Upload and process PDF sanctions list
// Content-Type: multipart/form-data
// Field: pdf

const uploadSanctionsPDF = async (file) => {
  const formData = new FormData();
  formData.append('pdf', file);

  const response = await fetch('/api/aml/upload-pdf', {
    method: 'POST',
    body: formData,
  });

  if (response.ok) {
    const result = await response.json();
    return result;
  } else {
    const error = await response.text();
    throw new Error(error);
  }
};

// Response format:
{
  "success": true,
  "processed": {
    "totalEntries": 1250,
    "persons": 850,
    "entities": 400,
    "duplicatesRemoved": 25
  },
  "categories": {
    "CDi": 320,
    "QDi": 180,
    "KPe": 150,
    "QDe": 120,
    "IQe": 100,
    "CDe": 80,
    "GBi": 75,
    "SOi": 65,
    "IQi": 60,
    "Generic": 90
  }
}
```

**POST /api/aml/upload-xml**
```javascript
// Upload and process XML sanctions data
// Content-Type: multipart/form-data
// Field: xml

const uploadSanctionsXML = async (file) => {
  const formData = new FormData();
  formData.append('xml', file);

  const response = await fetch('/api/aml/upload-xml', {
    method: 'POST',
    body: formData,
  });

  return await response.json();
};
```

**GET /api/aml/search**
```javascript
// Search sanctions database
// Query parameters: query, type, page, limit

const searchSanctions = async (searchQuery, options = {}) => {
  const params = new URLSearchParams({
    query: searchQuery,
    ...options
  });

  const response = await fetch(`/api/aml/search?${params}`);
  return await response.json();
};

// Response format:
{
  "results": [
    {
      "id": "CDi_001",
      "name": "John Doe",
      "type": "person",
      "category": "CDi",
      "nationality": "Unknown",
      "additionalInfo": "Additional details here"
    }
  ],
  "pagination": {
    "page": 1,
    "limit": 50,
    "total": 125,
    "pages": 3
  }
}
```

**POST /api/aml/process-risk-assessment**
```javascript
// Process client risk assessment Excel file
// Content-Type: multipart/form-data
// Field: excel

const processRiskAssessment = async (file) => {
  const formData = new FormData();
  formData.append('excel', file);

  const response = await fetch('/api/aml/process-risk-assessment', {
    method: 'POST',
    body: formData,
  });

  return await response.json();
};

// Response format:
{
  "success": true,
  "clients": [
    {
      "clientId": "CLIENT_001",
      "name": "ABC Corporation",
      "riskCategories": {
        "zoneGeographique": "Medium",
        "caracteristiquesClient": "Low",
        "reputationClient": "Low",
        "natureProduits": "Medium",
        "canalDistribution": "Low"
      },
      "overallRisk": "Medium",
      "riskScore": 65
    }
  ],
  "summary": {
    "totalClients": 150,
    "lowRisk": 80,
    "mediumRisk": 55,
    "highRisk": 15
  }
}
```

### **WebSocket API for Real-time Updates**

```javascript
// WebSocket connection for real-time updates
class CRC360WebSocket {
  constructor(url = 'ws://localhost:3000/ws') {
    this.url = url;
    this.ws = null;
    this.reconnectAttempts = 0;
    this.maxReconnectAttempts = 5;
    this.reconnectDelay = 1000;
    this.eventHandlers = new Map();
  }

  connect() {
    try {
      this.ws = new WebSocket(this.url);

      this.ws.onopen = () => {
        console.log('WebSocket connected');
        this.reconnectAttempts = 0;
        this.emit('connected');
      };

      this.ws.onmessage = (event) => {
        try {
          const data = JSON.parse(event.data);
          this.emit(data.type, data.payload);
        } catch (error) {
          console.error('Failed to parse WebSocket message:', error);
        }
      };

      this.ws.onclose = () => {
        console.log('WebSocket disconnected');
        this.emit('disconnected');
        this.attemptReconnect();
      };

      this.ws.onerror = (error) => {
        console.error('WebSocket error:', error);
        this.emit('error', error);
      };

    } catch (error) {
      console.error('Failed to connect WebSocket:', error);
      this.attemptReconnect();
    }
  }

  attemptReconnect() {
    if (this.reconnectAttempts < this.maxReconnectAttempts) {
      this.reconnectAttempts++;
      const delay = this.reconnectDelay * Math.pow(2, this.reconnectAttempts - 1);

      setTimeout(() => {
        console.log(`Attempting to reconnect (${this.reconnectAttempts}/${this.maxReconnectAttempts})`);
        this.connect();
      }, delay);
    }
  }

  on(event, handler) {
    if (!this.eventHandlers.has(event)) {
      this.eventHandlers.set(event, []);
    }
    this.eventHandlers.get(event).push(handler);
  }

  emit(event, data) {
    const handlers = this.eventHandlers.get(event);
    if (handlers) {
      handlers.forEach(handler => handler(data));
    }
  }

  send(type, payload) {
    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
      this.ws.send(JSON.stringify({ type, payload }));
    } else {
      console.warn('WebSocket not connected');
    }
  }

  disconnect() {
    if (this.ws) {
      this.ws.close();
      this.ws = null;
    }
  }
}

// Usage example:
const ws = new CRC360WebSocket();

// Listen for events
ws.on('document_uploaded', (data) => {
  console.log('New document uploaded:', data);
  // Update UI
});

ws.on('aml_processing_complete', (data) => {
  console.log('AML processing completed:', data);
  // Refresh AML data
});

ws.on('system_alert', (data) => {
  console.log('System alert:', data);
  // Show notification
});

// Connect
ws.connect();
```

### **SDK and Client Libraries**

#### **JavaScript/TypeScript SDK**
```typescript
// crc360-sdk.ts
export interface CRC360Config {
  baseUrl: string;
  apiKey?: string;
  timeout?: number;
}

export interface Document {
  id: number;
  title: string;
  category: string;
  description?: string;
  filename: string;
  size: number;
  uploadDate: string;
  lastModified: string;
}

export interface UploadResult {
  success: boolean;
  document?: Document;
  error?: string;
}

export class CRC360SDK {
  private config: CRC360Config;
  private baseHeaders: Record<string, string>;

  constructor(config: CRC360Config) {
    this.config = {
      timeout: 30000,
      ...config
    };

    this.baseHeaders = {
      'Content-Type': 'application/json',
    };

    if (config.apiKey) {
      this.baseHeaders['Authorization'] = `Bearer ${config.apiKey}`;
    }
  }

  private async request<T>(
    endpoint: string,
    options: RequestInit = {}
  ): Promise<T> {
    const url = `${this.config.baseUrl}${endpoint}`;

    const response = await fetch(url, {
      ...options,
      headers: {
        ...this.baseHeaders,
        ...options.headers,
      },
      signal: AbortSignal.timeout(this.config.timeout!),
    });

    if (!response.ok) {
      throw new Error(`HTTP ${response.status}: ${response.statusText}`);
    }

    return await response.json();
  }

  // Document management methods
  async uploadDocument(
    file: File,
    metadata: {
      title: string;
      category: string;
      description?: string;
    }
  ): Promise<UploadResult> {
    const formData = new FormData();
    formData.append('file', file);
    formData.append('title', metadata.title);
    formData.append('category', metadata.category);
    if (metadata.description) {
      formData.append('description', metadata.description);
    }

    const response = await fetch(`${this.config.baseUrl}/api/docsecure/upload`, {
      method: 'POST',
      headers: {
        ...this.baseHeaders,
        'Content-Type': undefined, // Let browser set multipart boundary
      },
      body: formData,
    });

    return await response.json();
  }

  async getDocuments(filters?: {
    category?: string;
    search?: string;
    page?: number;
    limit?: number;
  }): Promise<{ documents: Document[]; pagination: any }> {
    const params = new URLSearchParams();
    if (filters) {
      Object.entries(filters).forEach(([key, value]) => {
        if (value !== undefined) {
          params.append(key, value.toString());
        }
      });
    }

    return this.request(`/api/docsecure/documents?${params}`);
  }

  async downloadDocument(id: number): Promise<Blob> {
    const response = await fetch(`${this.config.baseUrl}/api/docsecure/download/${id}`, {
      headers: this.baseHeaders,
    });

    if (!response.ok) {
      throw new Error(`Download failed: ${response.statusText}`);
    }

    return await response.blob();
  }

  // AML methods
  async uploadSanctionsPDF(file: File): Promise<any> {
    const formData = new FormData();
    formData.append('pdf', file);

    const response = await fetch(`${this.config.baseUrl}/api/aml/upload-pdf`, {
      method: 'POST',
      body: formData,
    });

    return await response.json();
  }

  async searchSanctions(query: string, options?: {
    type?: string;
    page?: number;
    limit?: number;
  }): Promise<any> {
    const params = new URLSearchParams({ query });
    if (options) {
      Object.entries(options).forEach(([key, value]) => {
        if (value !== undefined) {
          params.append(key, value.toString());
        }
      });
    }

    return this.request(`/api/aml/search?${params}`);
  }

  // System methods
  async getHealth(): Promise<any> {
    return this.request('/api/health');
  }

  async authenticate(username: string, password: string): Promise<any> {
    return this.request('/api/admin/auth/login', {
      method: 'POST',
      body: JSON.stringify({ username, password }),
    });
  }
}

// Usage example:
const sdk = new CRC360SDK({
  baseUrl: 'https://crc360.bcpsecurities.ma',
  apiKey: 'your-api-key-here'
});

// Upload a document
const file = new File(['content'], 'test.pdf', { type: 'application/pdf' });
const result = await sdk.uploadDocument(file, {
  title: 'Test Document',
  category: 'Proc√©dures',
  description: 'Test document upload'
});

// Search documents
const documents = await sdk.getDocuments({
  category: 'Proc√©dures',
  search: 'risk',
  page: 1,
  limit: 20
});
```

#### **Python SDK**
```python
# crc360_sdk.py
import requests
import json
from typing import Optional, Dict, Any, List
from pathlib import Path

class CRC360SDK:
    def __init__(self, base_url: str, api_key: Optional[str] = None, timeout: int = 30):
        self.base_url = base_url.rstrip('/')
        self.timeout = timeout
        self.session = requests.Session()

        if api_key:
            self.session.headers.update({'Authorization': f'Bearer {api_key}'})

    def _request(self, method: str, endpoint: str, **kwargs) -> Dict[Any, Any]:
        """Make HTTP request to API"""
        url = f"{self.base_url}{endpoint}"

        try:
            response = self.session.request(
                method=method,
                url=url,
                timeout=self.timeout,
                **kwargs
            )
            response.raise_for_status()
            return response.json()
        except requests.exceptions.RequestException as e:
            raise Exception(f"API request failed: {e}")

    def upload_document(self, file_path: str, title: str, category: str,
                       description: Optional[str] = None) -> Dict[Any, Any]:
        """Upload document to Doc Secure"""
        file_path = Path(file_path)

        if not file_path.exists():
            raise FileNotFoundError(f"File not found: {file_path}")

        with open(file_path, 'rb') as f:
            files = {'file': (file_path.name, f, 'application/octet-stream')}
            data = {
                'title': title,
                'category': category
            }
            if description:
                data['description'] = description

            response = self.session.post(
                f"{self.base_url}/api/docsecure/upload",
                files=files,
                data=data,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json()

    def get_documents(self, category: Optional[str] = None,
                     search: Optional[str] = None,
                     page: int = 1, limit: int = 20) -> Dict[Any, Any]:
        """Get documents with optional filtering"""
        params = {'page': page, 'limit': limit}

        if category:
            params['category'] = category
        if search:
            params['search'] = search

        return self._request('GET', '/api/docsecure/documents', params=params)

    def download_document(self, document_id: int, save_path: str) -> None:
        """Download document by ID"""
        response = self.session.get(
            f"{self.base_url}/api/docsecure/download/{document_id}",
            timeout=self.timeout
        )
        response.raise_for_status()

        with open(save_path, 'wb') as f:
            f.write(response.content)

    def upload_sanctions_pdf(self, file_path: str) -> Dict[Any, Any]:
        """Upload and process sanctions PDF"""
        file_path = Path(file_path)

        with open(file_path, 'rb') as f:
            files = {'pdf': (file_path.name, f, 'application/pdf')}

            response = self.session.post(
                f"{self.base_url}/api/aml/upload-pdf",
                files=files,
                timeout=self.timeout
            )
            response.raise_for_status()
            return response.json()

    def search_sanctions(self, query: str, type_filter: Optional[str] = None,
                        page: int = 1, limit: int = 50) -> Dict[Any, Any]:
        """Search sanctions database"""
        params = {
            'query': query,
            'page': page,
            'limit': limit
        }

        if type_filter:
            params['type'] = type_filter

        return self._request('GET', '/api/aml/search', params=params)

    def get_health(self) -> Dict[Any, Any]:
        """Get system health status"""
        return self._request('GET', '/api/health')

    def authenticate(self, username: str, password: str) -> Dict[Any, Any]:
        """Authenticate with the system"""
        data = {
            'username': username,
            'password': password
        }

        result = self._request('POST', '/api/admin/auth/login', json=data)

        # Update session with token
        if 'token' in result:
            self.session.headers.update({
                'Authorization': f"Bearer {result['token']}"
            })

        return result

# Usage example:
if __name__ == "__main__":
    # Initialize SDK
    sdk = CRC360SDK('https://crc360.bcpsecurities.ma')

    # Authenticate
    auth_result = sdk.authenticate('admin', 'password')
    print(f"Authenticated: {auth_result}")

    # Upload document
    upload_result = sdk.upload_document(
        file_path='./test_document.pdf',
        title='Test Document',
        category='Proc√©dures',
        description='Test document upload via SDK'
    )
    print(f"Upload result: {upload_result}")

    # Search documents
    documents = sdk.get_documents(category='Proc√©dures', search='test')
    print(f"Found {len(documents['documents'])} documents")

    # Check system health
    health = sdk.get_health()
    print(f"System status: {health['status']}")
```

## ü§ù Contributing and Development Guidelines

### **Development Environment Setup**

#### **Prerequisites for Contributors**
```bash
# Required software versions
node --version    # v20.0.0 or higher
npm --version     # v10.0.0 or higher
git --version     # v2.30.0 or higher
python --version  # v3.9.0 or higher (for Doc Secure)

# Recommended tools
code --version    # VS Code (recommended IDE)
docker --version  # For containerized development
```

#### **Development Setup Process**
```bash
# 1. Fork and clone the repository
git clone https://github.com/yourusername/CRC360.git
cd CRC360

# 2. Install dependencies
npm install

# 3. Set up environment variables
cp .env.example .env.local
# Edit .env.local with your configuration

# 4. Initialize development database
python scripts/init_docsecure.py

# 5. Start development server
npm run dev

# 6. Verify setup
curl http://localhost:3000/api/health
```

#### **IDE Configuration**

**VS Code Settings (`.vscode/settings.json`)**
```json
{
  "typescript.preferences.importModuleSpecifier": "relative",
  "editor.formatOnSave": true,
  "editor.codeActionsOnSave": {
    "source.fixAll.eslint": true,
    "source.organizeImports": true
  },
  "files.associations": {
    "*.css": "tailwindcss"
  },
  "emmet.includeLanguages": {
    "javascript": "javascriptreact",
    "typescript": "typescriptreact"
  },
  "tailwindCSS.experimental.classRegex": [
    ["cva\\(([^)]*)\\)", "[\"'`]([^\"'`]*).*?[\"'`]"],
    ["cx\\(([^)]*)\\)", "(?:'|\"|`)([^']*)(?:'|\"|`)"]
  ]
}
```

**Recommended VS Code Extensions**
```json
// .vscode/extensions.json
{
  "recommendations": [
    "bradlc.vscode-tailwindcss",
    "esbenp.prettier-vscode",
    "dbaeumer.vscode-eslint",
    "ms-vscode.vscode-typescript-next",
    "formulahendry.auto-rename-tag",
    "christian-kohler.path-intellisense",
    "ms-python.python",
    "ms-vscode.vscode-json"
  ]
}
```

### **Code Standards and Best Practices**

#### **TypeScript/JavaScript Guidelines**

**Naming Conventions**
```typescript
// ‚úÖ Good examples
// Components: PascalCase
const DocumentUploader = () => { ... };
const AMLSearchResults = () => { ... };

// Functions and variables: camelCase
const processDocument = async (file: File) => { ... };
const isAuthenticated = true;

// Constants: SCREAMING_SNAKE_CASE
const MAX_FILE_SIZE = 50 * 1024 * 1024;
const API_ENDPOINTS = {
  UPLOAD: '/api/docsecure/upload',
  SEARCH: '/api/aml/search'
};

// Types and interfaces: PascalCase
interface DocumentMetadata {
  title: string;
  category: string;
  description?: string;
}

type UploadStatus = 'pending' | 'uploading' | 'success' | 'error';

// ‚ùå Bad examples
const document_uploader = () => { ... };  // Wrong case
const ISAUTH = true;                      // Not descriptive
const maxsize = 1000;                     // Not clear
```

**Function Documentation**
```typescript
/**
 * Uploads a document to the Doc Secure system
 *
 * @param file - The file to upload
 * @param metadata - Document metadata including title and category
 * @param options - Optional upload configuration
 * @returns Promise resolving to upload result
 *
 * @example
 * ```typescript
 * const result = await uploadDocument(file, {
 *   title: 'Risk Assessment',
 *   category: 'Proc√©dures'
 * });
 * ```
 *
 * @throws {Error} When file validation fails
 * @throws {NetworkError} When upload request fails
 */
async function uploadDocument(
  file: File,
  metadata: DocumentMetadata,
  options: UploadOptions = {}
): Promise<UploadResult> {
  // Implementation...
}
```

**Error Handling Patterns**
```typescript
// ‚úÖ Proper error handling
class DocumentService {
  async uploadDocument(file: File, metadata: DocumentMetadata): Promise<UploadResult> {
    try {
      // Validate input
      this.validateFile(file);
      this.validateMetadata(metadata);

      // Perform upload
      const result = await this.performUpload(file, metadata);

      // Log success
      logger.info('Document uploaded successfully', {
        filename: file.name,
        size: file.size,
        category: metadata.category
      });

      return result;

    } catch (error) {
      // Log error with context
      logger.error('Document upload failed', {
        filename: file.name,
        error: error.message,
        stack: error.stack
      });

      // Re-throw with user-friendly message
      if (error instanceof ValidationError) {
        throw new Error(`Invalid file: ${error.message}`);
      } else if (error instanceof NetworkError) {
        throw new Error('Upload failed due to network issues. Please try again.');
      } else {
        throw new Error('An unexpected error occurred during upload.');
      }
    }
  }

  private validateFile(file: File): void {
    if (!file) {
      throw new ValidationError('File is required');
    }

    if (file.size > MAX_FILE_SIZE) {
      throw new ValidationError(`File size exceeds ${MAX_FILE_SIZE / 1024 / 1024}MB limit`);
    }

    if (!ALLOWED_MIME_TYPES.includes(file.type)) {
      throw new ValidationError(`File type ${file.type} is not allowed`);
    }
  }
}
```

#### **React Component Guidelines**

**Component Structure**
```typescript
// ‚úÖ Well-structured component
import React, { useState, useEffect, useCallback } from 'react';
import { Button } from '@/components/ui/button';
import { Card, CardContent, CardHeader, CardTitle } from '@/components/ui/card';
import { useToast } from '@/hooks/use-toast';
import { DocumentService } from '@/lib/services/document-service';

interface DocumentUploaderProps {
  /** Allowed file categories */
  allowedCategories: string[];
  /** Maximum file size in bytes */
  maxFileSize?: number;
  /** Callback fired when upload completes */
  onUploadComplete?: (result: UploadResult) => void;
  /** Whether the uploader is disabled */
  disabled?: boolean;
}

/**
 * Document uploader component for Doc Secure module
 *
 * Provides drag-and-drop file upload with validation,
 * progress tracking, and error handling.
 */
export const DocumentUploader: React.FC<DocumentUploaderProps> = ({
  allowedCategories,
  maxFileSize = 50 * 1024 * 1024,
  onUploadComplete,
  disabled = false
}) => {
  // State management
  const [isUploading, setIsUploading] = useState(false);
  const [uploadProgress, setUploadProgress] = useState(0);
  const [selectedFile, setSelectedFile] = useState<File | null>(null);

  // Hooks
  const { toast } = useToast();

  // Memoized callbacks
  const handleFileSelect = useCallback((file: File) => {
    if (file.size > maxFileSize) {
      toast({
        title: 'File too large',
        description: `Maximum file size is ${maxFileSize / 1024 / 1024}MB`,
        variant: 'destructive'
      });
      return;
    }

    setSelectedFile(file);
  }, [maxFileSize, toast]);

  const handleUpload = useCallback(async (metadata: DocumentMetadata) => {
    if (!selectedFile) return;

    setIsUploading(true);
    setUploadProgress(0);

    try {
      const result = await DocumentService.upload(selectedFile, metadata, {
        onProgress: setUploadProgress
      });

      toast({
        title: 'Upload successful',
        description: `${selectedFile.name} has been uploaded successfully`
      });

      onUploadComplete?.(result);
      setSelectedFile(null);

    } catch (error) {
      toast({
        title: 'Upload failed',
        description: error.message,
        variant: 'destructive'
      });
    } finally {
      setIsUploading(false);
      setUploadProgress(0);
    }
  }, [selectedFile, onUploadComplete, toast]);

  // Render
  return (
    <Card className="w-full max-w-md">
      <CardHeader>
        <CardTitle>Upload Document</CardTitle>
      </CardHeader>
      <CardContent>
        {/* Component content */}
      </CardContent>
    </Card>
  );
};

// Default props and display name
DocumentUploader.displayName = 'DocumentUploader';
```

**Custom Hooks Pattern**
```typescript
// ‚úÖ Custom hook for document management
import { useState, useEffect, useCallback } from 'react';
import { DocumentService } from '@/lib/services/document-service';

interface UseDocumentsOptions {
  category?: string;
  autoRefresh?: boolean;
  refreshInterval?: number;
}

interface UseDocumentsReturn {
  documents: Document[];
  loading: boolean;
  error: string | null;
  refresh: () => Promise<void>;
  uploadDocument: (file: File, metadata: DocumentMetadata) => Promise<void>;
  deleteDocument: (id: number) => Promise<void>;
}

export const useDocuments = (options: UseDocumentsOptions = {}): UseDocumentsReturn => {
  const { category, autoRefresh = false, refreshInterval = 30000 } = options;

  const [documents, setDocuments] = useState<Document[]>([]);
  const [loading, setLoading] = useState(true);
  const [error, setError] = useState<string | null>(null);

  const fetchDocuments = useCallback(async () => {
    try {
      setLoading(true);
      setError(null);

      const result = await DocumentService.getDocuments({ category });
      setDocuments(result.documents);

    } catch (err) {
      setError(err.message);
    } finally {
      setLoading(false);
    }
  }, [category]);

  const uploadDocument = useCallback(async (file: File, metadata: DocumentMetadata) => {
    const result = await DocumentService.upload(file, metadata);
    await fetchDocuments(); // Refresh list
    return result;
  }, [fetchDocuments]);

  const deleteDocument = useCallback(async (id: number) => {
    await DocumentService.delete(id);
    await fetchDocuments(); // Refresh list
  }, [fetchDocuments]);

  // Initial load
  useEffect(() => {
    fetchDocuments();
  }, [fetchDocuments]);

  // Auto-refresh
  useEffect(() => {
    if (!autoRefresh) return;

    const interval = setInterval(fetchDocuments, refreshInterval);
    return () => clearInterval(interval);
  }, [autoRefresh, refreshInterval, fetchDocuments]);

  return {
    documents,
    loading,
    error,
    refresh: fetchDocuments,
    uploadDocument,
    deleteDocument
  };
};
```

### **Testing Guidelines**

#### **Unit Testing with Jest and React Testing Library**
```typescript
// __tests__/components/DocumentUploader.test.tsx
import React from 'react';
import { render, screen, fireEvent, waitFor } from '@testing-library/react';
import userEvent from '@testing-library/user-event';
import { DocumentUploader } from '@/components/DocumentUploader';
import { DocumentService } from '@/lib/services/document-service';

// Mock the service
jest.mock('@/lib/services/document-service');
const mockDocumentService = DocumentService as jest.Mocked<typeof DocumentService>;

describe('DocumentUploader', () => {
  const defaultProps = {
    allowedCategories: ['Proc√©dures', 'Politiques'],
    maxFileSize: 10 * 1024 * 1024, // 10MB
  };

  beforeEach(() => {
    jest.clearAllMocks();
  });

  it('renders upload interface correctly', () => {
    render(<DocumentUploader {...defaultProps} />);

    expect(screen.getByText('Upload Document')).toBeInTheDocument();
    expect(screen.getByRole('button', { name: /choose file/i })).toBeInTheDocument();
  });

  it('validates file size correctly', async () => {
    const user = userEvent.setup();
    render(<DocumentUploader {...defaultProps} />);

    // Create a large file
    const largeFile = new File(['x'.repeat(20 * 1024 * 1024)], 'large.pdf', {
      type: 'application/pdf'
    });

    const fileInput = screen.getByLabelText(/choose file/i);
    await user.upload(fileInput, largeFile);

    expect(screen.getByText(/file too large/i)).toBeInTheDocument();
  });

  it('uploads file successfully', async () => {
    const user = userEvent.setup();
    const onUploadComplete = jest.fn();

    mockDocumentService.upload.mockResolvedValue({
      success: true,
      document: { id: 1, title: 'Test Document' }
    });

    render(
      <DocumentUploader
        {...defaultProps}
        onUploadComplete={onUploadComplete}
      />
    );

    // Upload file
    const file = new File(['content'], 'test.pdf', { type: 'application/pdf' });
    const fileInput = screen.getByLabelText(/choose file/i);
    await user.upload(fileInput, file);

    // Fill metadata
    await user.type(screen.getByLabelText(/title/i), 'Test Document');
    await user.selectOptions(screen.getByLabelText(/category/i), 'Proc√©dures');

    // Submit
    await user.click(screen.getByRole('button', { name: /upload/i }));

    await waitFor(() => {
      expect(mockDocumentService.upload).toHaveBeenCalledWith(
        file,
        expect.objectContaining({
          title: 'Test Document',
          category: 'Proc√©dures'
        }),
        expect.any(Object)
      );
    });

    expect(onUploadComplete).toHaveBeenCalledWith(
      expect.objectContaining({ success: true })
    );
  });

  it('handles upload errors gracefully', async () => {
    const user = userEvent.setup();

    mockDocumentService.upload.mockRejectedValue(
      new Error('Network error')
    );

    render(<DocumentUploader {...defaultProps} />);

    // Upload file and submit
    const file = new File(['content'], 'test.pdf', { type: 'application/pdf' });
    const fileInput = screen.getByLabelText(/choose file/i);
    await user.upload(fileInput, file);

    await user.type(screen.getByLabelText(/title/i), 'Test Document');
    await user.selectOptions(screen.getByLabelText(/category/i), 'Proc√©dures');
    await user.click(screen.getByRole('button', { name: /upload/i }));

    await waitFor(() => {
      expect(screen.getByText(/upload failed/i)).toBeInTheDocument();
    });
  });
});
```

#### **Integration Testing**
```typescript
// __tests__/integration/document-workflow.test.tsx
import { render, screen, waitFor } from '@testing-library/react';
import userEvent from '@testing-library/user-event';
import { DocumentManagement } from '@/pages/docsecure/documents';
import { setupServer } from 'msw/node';
import { rest } from 'msw';

// Mock API server
const server = setupServer(
  rest.get('/api/docsecure/documents', (req, res, ctx) => {
    return res(
      ctx.json({
        documents: [
          {
            id: 1,
            title: 'Test Document',
            category: 'Proc√©dures',
            filename: 'test.pdf',
            size: 1024,
            uploadDate: '2025-01-15T10:00:00Z'
          }
        ],
        pagination: { page: 1, total: 1, pages: 1 }
      })
    );
  }),

  rest.post('/api/docsecure/upload', (req, res, ctx) => {
    return res(
      ctx.json({
        success: true,
        document: {
          id: 2,
          title: 'New Document',
          category: 'Proc√©dures'
        }
      })
    );
  })
);

beforeAll(() => server.listen());
afterEach(() => server.resetHandlers());
afterAll(() => server.close());

describe('Document Management Workflow', () => {
  it('completes full document lifecycle', async () => {
    const user = userEvent.setup();
    render(<DocumentManagement />);

    // Wait for initial load
    await waitFor(() => {
      expect(screen.getByText('Test Document')).toBeInTheDocument();
    });

    // Upload new document
    const uploadButton = screen.getByRole('button', { name: /upload/i });
    await user.click(uploadButton);

    // Fill upload form
    const file = new File(['content'], 'new.pdf', { type: 'application/pdf' });
    const fileInput = screen.getByLabelText(/file/i);
    await user.upload(fileInput, file);

    await user.type(screen.getByLabelText(/title/i), 'New Document');
    await user.selectOptions(screen.getByLabelText(/category/i), 'Proc√©dures');

    // Submit upload
    await user.click(screen.getByRole('button', { name: /submit/i }));

    // Verify success
    await waitFor(() => {
      expect(screen.getByText(/upload successful/i)).toBeInTheDocument();
    });
  });
});
```

### **Git Workflow and Contribution Process**

#### **Branch Naming Convention**
```bash
# Feature branches
feature/add-document-versioning
feature/improve-aml-processing
feature/enhance-security

# Bug fix branches
bugfix/fix-upload-timeout
bugfix/resolve-search-pagination
bugfix/correct-date-formatting

# Hotfix branches (for production issues)
hotfix/critical-security-patch
hotfix/fix-database-connection

# Release branches
release/v1.2.0
release/v1.2.1
```

#### **Commit Message Format**
```bash
# Format: <type>(<scope>): <description>
#
# Types: feat, fix, docs, style, refactor, test, chore
# Scope: module or component affected
# Description: brief description of changes

# Examples:
feat(docsecure): add document versioning support
fix(aml): resolve PDF parsing timeout issue
docs(readme): update installation instructions
style(ui): improve button hover states
refactor(api): extract common validation logic
test(upload): add integration tests for file upload
chore(deps): update dependencies to latest versions

# For breaking changes:
feat(api)!: change document upload response format

BREAKING CHANGE: The upload API now returns a different response structure.
Migration guide: Update your code to use the new 'document' field instead of 'file'.
```

#### **Pull Request Process**
```markdown
## Pull Request Template

### Description
Brief description of changes and motivation.

### Type of Change
- [ ] Bug fix (non-breaking change which fixes an issue)
- [ ] New feature (non-breaking change which adds functionality)
- [ ] Breaking change (fix or feature that would cause existing functionality to not work as expected)
- [ ] Documentation update

### Testing
- [ ] Unit tests pass
- [ ] Integration tests pass
- [ ] Manual testing completed
- [ ] Performance impact assessed

### Checklist
- [ ] Code follows project style guidelines
- [ ] Self-review completed
- [ ] Code is commented where necessary
- [ ] Documentation updated
- [ ] No new warnings introduced
- [ ] Tests added for new functionality

### Screenshots (if applicable)
Add screenshots to help explain your changes.

### Related Issues
Closes #123
Relates to #456
```

#### **Code Review Guidelines**

**For Reviewers:**
```markdown
## Code Review Checklist

### Functionality
- [ ] Code does what it's supposed to do
- [ ] Edge cases are handled
- [ ] Error handling is appropriate
- [ ] Performance is acceptable

### Code Quality
- [ ] Code is readable and well-structured
- [ ] Functions are appropriately sized
- [ ] Variable names are descriptive
- [ ] Comments explain complex logic

### Security
- [ ] Input validation is present
- [ ] No sensitive data in logs
- [ ] Authentication/authorization is correct
- [ ] SQL injection prevention

### Testing
- [ ] Adequate test coverage
- [ ] Tests are meaningful
- [ ] Tests pass consistently
- [ ] Integration points tested

### Documentation
- [ ] README updated if needed
- [ ] API documentation updated
- [ ] Code comments are helpful
- [ ] Breaking changes documented
```

**Review Comments Examples:**
```typescript
// ‚úÖ Good review comments
// "Consider extracting this logic into a separate function for reusability"
// "This could cause a memory leak if the component unmounts during the async operation"
// "Great error handling! Consider adding a user-friendly message here"
// "This function is doing too much - could we split it into smaller functions?"

// ‚ùå Poor review comments
// "This is wrong"
// "Fix this"
// "Bad code"
// "I don't like this"
```

## üìû Support and Community

### **Getting Help and Support**

#### **Official Support Channels**

**Primary Support**
- **Email**: support@bcpsecurities.ma
- **Response Time**: 24-48 hours for general inquiries
- **Emergency Support**: emergency@bcpsecurities.ma (for critical issues)
- **Phone**: +212 5XX-XXX-XXX (Business hours: 9 AM - 6 PM GMT+1)

**Documentation and Resources**
- **Official Documentation**: [https://docs.crc360.bcpsecurities.ma](https://docs.crc360.bcpsecurities.ma)
- **API Reference**: [https://api.crc360.bcpsecurities.ma/docs](https://api.crc360.bcpsecurities.ma/docs)
- **Video Tutorials**: [https://training.bcpsecurities.ma/crc360](https://training.bcpsecurities.ma/crc360)
- **Knowledge Base**: [https://kb.bcpsecurities.ma](https://kb.bcpsecurities.ma)

**Community Platforms**
- **GitHub Issues**: [https://github.com/gitsupportb/CRC360/issues](https://github.com/gitsupportb/CRC360/issues)
- **GitHub Discussions**: [https://github.com/gitsupportb/CRC360/discussions](https://github.com/gitsupportb/CRC360/discussions)
- **Internal Slack**: #crc360-support (for BCP Securities Services employees)
- **User Forum**: [https://forum.bcpsecurities.ma/crc360](https://forum.bcpsecurities.ma/crc360)

#### **Support Request Guidelines**

**When Reporting Issues**
```markdown
## Issue Report Template

### Environment Information
- **CRC360 Version**: [e.g., v1.2.3]
- **Browser**: [e.g., Chrome 120.0.6099.109]
- **Operating System**: [e.g., Windows 11, macOS 14.2]
- **Node.js Version**: [e.g., v20.10.0] (for development issues)

### Issue Description
**Summary**: Brief description of the issue

**Expected Behavior**: What should happen

**Actual Behavior**: What actually happens

**Steps to Reproduce**:
1. Go to '...'
2. Click on '...'
3. Scroll down to '...'
4. See error

### Additional Context
**Screenshots**: Add screenshots if applicable

**Error Messages**: Include any error messages or console logs

**Frequency**: How often does this occur?
- [ ] Always
- [ ] Sometimes
- [ ] Rarely
- [ ] First time

**Impact Level**:
- [ ] Critical (system unusable)
- [ ] High (major functionality affected)
- [ ] Medium (minor functionality affected)
- [ ] Low (cosmetic issue)

### Attempted Solutions
List any troubleshooting steps you've already tried.
```

**Feature Request Template**
```markdown
## Feature Request Template

### Feature Summary
Brief description of the requested feature.

### Problem Statement
What problem does this feature solve?

### Proposed Solution
Detailed description of how you envision this feature working.

### Alternative Solutions
Any alternative approaches you've considered.

### Use Cases
Specific scenarios where this feature would be beneficial.

### Priority
- [ ] Critical (blocking current work)
- [ ] High (would significantly improve workflow)
- [ ] Medium (nice to have)
- [ ] Low (minor improvement)

### Additional Context
Any other context, mockups, or examples.
```

### **Training and Onboarding**

#### **New User Onboarding Program**

**Week 1: System Overview**
- Introduction to CRC360 architecture and modules
- Account setup and basic navigation
- Security policies and best practices
- Basic document management operations

**Week 2: Module Deep Dive**
- Rep Watch: Regulatory reporting workflows
- AML Center: Sanctions screening and risk assessment
- Doc Secure: Document management and organization
- R-Sense: Risk calculation and analytics

**Week 3: Advanced Features**
- API integration and automation
- Custom reporting and analytics
- System administration (for admin users)
- Troubleshooting common issues

**Week 4: Best Practices**
- Workflow optimization
- Security compliance
- Performance monitoring
- Backup and recovery procedures

#### **Training Materials**

**Interactive Tutorials**
```javascript
// Example: Interactive tutorial system
class CRC360Tutorial {
  constructor() {
    this.steps = [
      {
        target: '#main-dashboard',
        title: 'Welcome to CRC360',
        content: 'This is your main dashboard where you can access all modules.',
        position: 'bottom'
      },
      {
        target: '#rep-watch-card',
        title: 'Rep Watch Module',
        content: 'Click here to access regulatory reporting tools.',
        position: 'top'
      },
      {
        target: '#aml-center-card',
        title: 'AML Center',
        content: 'Manage sanctions screening and compliance here.',
        position: 'top'
      },
      {
        target: '#doc-secure-card',
        title: 'Doc Secure',
        content: 'Secure document management and storage.',
        position: 'top'
      },
      {
        target: '#r-sense-card',
        title: 'R-Sense',
        content: 'Advanced risk calculation and analytics.',
        position: 'top'
      }
    ];
  }

  start() {
    // Initialize tutorial overlay
    this.showStep(0);
  }

  showStep(index) {
    const step = this.steps[index];
    // Display tutorial step with highlighting and instructions
  }
}

// Usage in application
if (isFirstTimeUser()) {
  const tutorial = new CRC360Tutorial();
  tutorial.start();
}
```

**Video Training Series**
1. **Getting Started with CRC360** (15 minutes)
   - System overview and navigation
   - Basic security concepts
   - First-time setup

2. **Rep Watch Mastery** (25 minutes)
   - Regulatory reporting workflows
   - Calendar management
   - File upload and tracking

3. **AML Center Operations** (30 minutes)
   - Sanctions list management
   - Client risk assessment
   - Search and filtering techniques

4. **Doc Secure Best Practices** (20 minutes)
   - Document organization strategies
   - Security and access control
   - Search and retrieval optimization

5. **R-Sense Analytics** (35 minutes)
   - Risk calculation methodologies
   - Data import and export
   - Custom reporting

6. **System Administration** (40 minutes)
   - User management
   - System configuration
   - Monitoring and maintenance

### **Community Guidelines and Code of Conduct**

#### **Community Standards**

**Our Commitment**
We are committed to providing a welcoming and inclusive environment for all contributors, regardless of background, experience level, or personal characteristics.

**Expected Behavior**
- **Be Respectful**: Treat all community members with respect and courtesy
- **Be Collaborative**: Work together constructively and share knowledge
- **Be Professional**: Maintain professional standards in all interactions
- **Be Inclusive**: Welcome newcomers and help them get started
- **Be Constructive**: Provide helpful feedback and suggestions

**Unacceptable Behavior**
- Harassment, discrimination, or offensive language
- Personal attacks or inflammatory comments
- Spam, trolling, or disruptive behavior
- Sharing confidential or proprietary information
- Violating intellectual property rights

#### **Contribution Recognition**

**Contributor Levels**
```markdown
## Contributor Recognition Program

### üåü Community Contributor
- First-time contributors
- Documentation improvements
- Bug reports and feature requests
- Community support and help

### üöÄ Active Contributor
- Multiple merged pull requests
- Regular community participation
- Code reviews and feedback
- Testing and quality assurance

### üíé Core Contributor
- Significant feature contributions
- Mentoring other contributors
- Technical leadership
- Long-term project commitment

### üèÜ Maintainer
- Project governance participation
- Release management
- Security and compliance oversight
- Strategic planning and roadmap
```

**Recognition Methods**
- **GitHub Badges**: Special badges on GitHub profile
- **Hall of Fame**: Featured on project website
- **Conference Speaking**: Opportunities to present at events
- **Early Access**: Beta testing and preview features
- **Swag and Rewards**: Project merchandise and recognition items

### **Roadmap and Future Development**

#### **Short-term Goals (Q1-Q2 2025)**

**Performance Optimization**
- Database query optimization
- Frontend bundle size reduction
- Caching layer implementation
- Mobile responsiveness improvements

**Security Enhancements**
- Multi-factor authentication
- Advanced audit logging
- Penetration testing and remediation
- Compliance certification updates

**User Experience Improvements**
- Enhanced search functionality
- Improved navigation and workflows
- Accessibility compliance (WCAG 2.1)
- Dark mode support

#### **Medium-term Goals (Q3-Q4 2025)**

**Advanced Features**
- Machine learning for risk assessment
- Automated compliance monitoring
- Advanced analytics and reporting
- Integration with external systems

**Scalability Improvements**
- Microservices architecture
- Horizontal scaling capabilities
- Cloud-native deployment options
- API rate limiting and throttling

**International Expansion**
- Multi-language support
- Regional compliance modules
- Currency and timezone handling
- Localized documentation

#### **Long-term Vision (2026 and beyond)**

**AI and Automation**
- Intelligent document classification
- Predictive risk modeling
- Automated regulatory reporting
- Natural language processing for compliance

**Platform Evolution**
- Plugin architecture for extensibility
- Third-party integrations marketplace
- White-label solutions
- SaaS offering for smaller institutions

**Innovation Areas**
- Blockchain integration for audit trails
- Real-time collaboration features
- Advanced data visualization
- Mobile applications

### **Acknowledgments and Credits**

#### **Core Development Team**
- **Project Lead**: [Name] - Overall project direction and architecture
- **Frontend Lead**: [Name] - UI/UX design and React development
- **Backend Lead**: [Name] - Server architecture and API development
- **Security Lead**: [Name] - Security implementation and compliance
- **DevOps Lead**: [Name] - Infrastructure and deployment automation

#### **Contributors**
We thank all contributors who have helped make CRC360 possible:
- [Contributor names and their contributions]
- [Community members who provided feedback and testing]
- [Organizations that provided resources and support]

#### **Third-Party Acknowledgments**

**Open Source Libraries**
- **Next.js**: React framework for production
- **React**: JavaScript library for building user interfaces
- **Tailwind CSS**: Utility-first CSS framework
- **Radix UI**: Low-level UI primitives
- **Plotly.js**: JavaScript graphing library
- **Express.js**: Web application framework for Node.js
- **SQLite**: Self-contained SQL database engine
- **And many others** - See package.json for complete list

**Services and Tools**
- **GitHub**: Code hosting and collaboration
- **Vercel**: Deployment and hosting platform
- **npm**: Package management
- **Docker**: Containerization platform
- **Prometheus**: Monitoring and alerting
- **Grafana**: Analytics and monitoring dashboards

#### **Special Thanks**
- **BCP Securities Services**: For sponsoring and supporting the project
- **Regulatory Bodies**: For guidance on compliance requirements
- **Beta Testers**: For early feedback and testing
- **Community Members**: For ongoing support and contributions

---

## üìÑ License and Legal Information

### **License**
This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

### **Copyright**
Copyright ¬© 2025 BCP Securities Services. All rights reserved.

### **Disclaimer**
This software is provided "as is" without warranty of any kind. Users are responsible for ensuring compliance with applicable regulations and standards.

### **Contact Information**
For licensing inquiries or commercial use, please contact:
- **Email**: legal@bcpsecurities.ma
- **Address**: BCP Securities Services, [Address], Morocco

---

**üéâ Thank you for using CRC360! Together, we're building the future of risk management and compliance.**

---

*Last updated: January 15, 2025*
*Version: 1.0.0*
*Documentation version: 1.0.0*
